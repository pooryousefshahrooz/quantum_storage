{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"implement our OR model. When we want to code an optimization model,\n",
    "we put a placeholder for that model (like a blank canvas), \n",
    "then add its elements (decision variables and constraints) to it.\"\"\"\n",
    "import csv\n",
    "from network import Network\n",
    "from workload import Work_load\n",
    "# from docplex.mp.progress import *\n",
    "# from docplex.mp.progress import SolutionRecorder\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from docplex.mp.progress import *\n",
    "from docplex.mp.progress import SolutionRecorder\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPLEX_resource_cinsumption_minimization(network,work_load,life_time,iteration,cyclic_workload,window_size):\n",
    "    if cyclic_workload ==\"cyclic\":\n",
    "        cyclic_workload=True\n",
    "    else:\n",
    "        cyclic_workload= False\n",
    "    import docplex.mp.model as cpx\n",
    "    opt_model = cpx.Model(name=\"Storage problem model\"+str(iteration))\n",
    "    w_vars = {}\n",
    "    u_vars = {}\n",
    "    #opt_model = Model(name=\"Storage problem model\")\n",
    "#     for t in work_load.T:\n",
    "#         for k in work_load.each_t_requests[t]:\n",
    "#             for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "#     x_vars  = {(t,k,p): opt_model.integer_var(lb=0, ub= 100,\n",
    "#                               name=\"x_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T \n",
    "#                for k in work_load.each_t_requests[t] \n",
    "#                for p in network.each_request_virtual_paths[k]}\n",
    "    \n",
    "#     s_vars  = {(t,k,p): opt_model.integer_var(lb=0, ub= 100,\n",
    "#                               name=\"s_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T \n",
    "#                for k in work_load.each_t_requests[t] \n",
    "#                for p in network.each_request_real_paths[k]}\n",
    "    w_vars  = {(t,k,p): opt_model.continuous_var(lb=0, ub= network.max_edge_capacity,\n",
    "                              name=\"w_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T[:window_size] \n",
    "               for k in work_load.each_t_requests[t] \n",
    "               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]}\n",
    "\n",
    "    u_vars  = {(t,j,p): opt_model.continuous_var(lb=0, ub= network.max_edge_capacity,\n",
    "                                  name=\"u_{0}_{1}_{2}\".format(t,j,p))  for t in work_load.T[:window_size] \n",
    "                   for j in network.storage_pairs for p in network.each_request_real_paths[j]}   \n",
    "\n",
    "    if life_time ==1000:\n",
    "        #inventory evolution constraint\n",
    "        for t in work_load.T[1:window_size]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    \n",
    "                    \n",
    "#                     print(\"2for time %s and storage pair %s and real sub path %s\"%(t,j,p_s))\n",
    "#                     for k in work_load.each_t_requests[t]:\n",
    "#                         print(\"2this is the request \",k)\n",
    "#                         if k!=j:\n",
    "#                             print(\"2which was not equal to storage pair\")\n",
    "#                             for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "#                                 print(\"2this path %s include the sub path %s\"%(p,p_s))\n",
    "\n",
    "                    \n",
    "                    if cyclic_workload:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == u_vars[(t-1)%len(work_load.T),j,p_s]-\n",
    "                        opt_model.sum(w_vars[(t-1)%len(work_load.T),k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                        +opt_model.sum(w_vars[(t-1)%len(work_load.T),j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "                    else:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == u_vars[t-1,j,p_s]-\n",
    "                        opt_model.sum(w_vars[t-1,k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                        +opt_model.sum(w_vars[t-1,j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "    else:\n",
    "        #inventory evolution constraint\n",
    "        for t in work_load.T[1:window_size]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    \n",
    "                    if cyclic_workload:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == -\n",
    "                        opt_model.sum(w_vars[(t-1)%len(work_load.T),k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] \n",
    "                        )\n",
    "                        + opt_model.sum(w_vars[(t-1)%len(work_load.T),j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "                    else:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == -\n",
    "                        opt_model.sum(w_vars[t-1,k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] \n",
    "                        )\n",
    "                        + opt_model.sum(w_vars[t-1,j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "\n",
    "    # serving from inventory constraint\n",
    "    for t in work_load.T[1:window_size]:\n",
    "        for j in network.storage_pairs:\n",
    "            \n",
    "            for p_s in network.each_request_real_paths[j]:\n",
    "#                 print(\"for time %s and storage pair %s and real sub path %s\"%(t,j,p_s))\n",
    "#                 for k in work_load.each_t_requests[t]:\n",
    "#                     print(\"this is the request \",k)\n",
    "#                     if k!=j:\n",
    "#                         print(\"which was not equal to storage pair\")\n",
    "#                         for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "#                             print(\"this path %s include the sub path %s\"%(p,p_s))\n",
    "                            \n",
    "                opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]*network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] if k in list(network.each_request_virtual_paths_include_subpath.keys()))<=u_vars[t,j,p_s]\n",
    "                                     , ctname=\"inventory_serving_{0}_{1}_{2}\".format(t,j,p_s))  \n",
    " \n",
    "     \n",
    "    # Demand constriant\n",
    "    for t in work_load.T[1:window_size]:\n",
    "        for k in  work_load.each_t_requests[t]:\n",
    "#             print(\"work_load.each_t_each_request_demand\",work_load.each_t_each_request_demand)\n",
    "#             print(\"work_load.each_t_each_request_demand[t]\",work_load.each_t_each_request_demand[t])\n",
    "#             print(\"work_load.each_t_each_request_demand[t][k]\",work_load.each_t_each_request_demand[t][k])\n",
    "            opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]) >= \n",
    "                    work_load.each_t_each_request_demand[t][k], ctname=\"constraint_{0}_{1}\".format(t,k))\n",
    "    \n",
    "    #Edge constraint\n",
    "    for t in work_load.T[:window_size]:\n",
    "        for edge in network.set_E:\n",
    "            opt_model.add_constraint(\n",
    "                opt_model.sum(w_vars[t,k,p]*network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t)) for k in work_load.each_t_requests[t]\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k] if network.check_path_include_edge(edge,p))\n",
    "                                     \n",
    "                 <= network.each_edge_capacity[edge], ctname=\"edge_capacity_{0}_{1}\".format(t,edge))\n",
    "     \n",
    "    # storage servers capacity constraint\n",
    "    for t in work_load.T[:window_size]:\n",
    "        for s1 in network.storage_nodes:\n",
    "            opt_model.add_constraint(opt_model.sum(u_vars[t,(s1,s2),p] \n",
    "                for s2 in network.storage_nodes if network.check_storage_pair_exist(s1,s2)\n",
    "                for p in network.each_request_real_paths[(s1,s2)])\n",
    "        <= network.get_storage_capacity(s1), ctname=\"storage_capacity_constraint_{0}_{1}\".format(t,s1))\n",
    "    \n",
    "    # constraints for serving from storage at time zero and 1 should be zero\n",
    "    if not cyclic_workload:\n",
    "        for t in [0,1]:\n",
    "            opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                    for k in work_load.each_t_requests[t] for p in network.each_request_virtual_paths[k] \n",
    "                    )<=0, ctname=\"serving_from_inventory_{0}\".format(t))\n",
    "    \n",
    "    # constraints for putting in storage at time zero  should be zero\n",
    "    \"\"\"this is becasue we start the formulation from 1 and not from zero and we have t-1 in our formulation\"\"\"\n",
    "    for t in [0]:\n",
    "        opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                for k in work_load.each_t_requests[t] for p in network.each_request_real_paths[k] \n",
    "                )<=0, ctname=\"storing_in_inventory_{0}\".format(t))   \n",
    "    \n",
    "\n",
    "    # constraint for inventory is zero at time zero \n",
    "    if not cyclic_workload:\n",
    "        for t in [0]:\n",
    "            for j in network.storage_pairs:\n",
    "                 for p_s in network.each_request_real_paths[j]:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] <=0, ctname=\"storage_capacity_constraint_{0}_{1}_{2}\".format(t,j,p_s))\n",
    "    \n",
    "    \"\"\"defining an objective, which is a linear expression\"\"\"\n",
    "#     objective = opt_model.sum(1/len(work_load.T[1:])*1/len(work_load.each_t_real_requests[t])*1/work_load.each_t_each_request_demand[t][k]\n",
    "#                               *(w_vars[t,k,p] * network.get_path_length(p)) for t in work_load.T[1:]\n",
    "#                               for k in work_load.each_t_real_requests[t] \n",
    "#                               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]\n",
    "#                               )\n",
    "    objective = opt_model.sum(1/len(work_load.T[1:window_size])*1/len(work_load.each_t_real_requests[t])*1/work_load.each_t_each_request_demand[t][k]\n",
    "                              *(w_vars[t,k,p] * network.get_path_length(p)) for t in work_load.T[1:window_size]\n",
    "                              for k in work_load.each_t_real_requests[t] \n",
    "                              for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]\n",
    "                              )\n",
    "#     opt_model.minimize(1/len(work_load.T[1:])*\n",
    "#                     1/opt_model.sum(len(work_load.each_t_real_requests[t]) for t in work_load.T[1:])*\n",
    "#                     (opt_model.sum(w_vars[t,k,p] * network.get_path_length(p)/work_load.each_t_each_request_demand[t][k] for t in work_load.T[1:]\n",
    "#                               for k in work_load.each_t_real_requests[t] \n",
    "#                               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]))\n",
    "#                                 )\n",
    "    \n",
    "    # for maximization\n",
    "    opt_model.minimize(objective)\n",
    "    \n",
    "#     opt_model.solve()\n",
    "    opt_model.print_information()\n",
    "    #try:\n",
    "    opt_model.solve()\n",
    "\n",
    "\n",
    "    \n",
    "    print('docplex.mp.solution',opt_model.solution)\n",
    "    objective_value = -1\n",
    "    try:\n",
    "        if opt_model.solution:\n",
    "            objective_value =opt_model.solution.get_objective_value()\n",
    "    except ValueError:\n",
    "        print(ValueError)\n",
    "        #pass\n",
    "    #pdb.set_trace()\n",
    "    w_variable_value = {}\n",
    "    u_variable_value = {}\n",
    "    if objective_value>0:\n",
    "        for t in work_load.T[:window_size]:\n",
    "             for k in work_load.each_t_requests[t]:\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                    try:\n",
    "                        w_variable_value[t][k][p] = w_vars[t,k,p].solution_value\n",
    "                    except:\n",
    "                        try:\n",
    "                            w_variable_value[t][k]={}\n",
    "                            w_variable_value[t][k][p] = w_vars[t,k,p].solution_value\n",
    "                        except:\n",
    "                            try:\n",
    "                                w_variable_value[t]={}\n",
    "                                w_variable_value[t][k]={}\n",
    "                                w_variable_value[t][k][p] = w_vars[t,k,p].solution_value\n",
    "                            except ValueError:\n",
    "                                print(ValueError)\n",
    "\n",
    "        for t in work_load.T[:window_size]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p in network.each_request_real_paths[j]: \n",
    "                    try:\n",
    "                        u_variable_value[t][j][p] = u_vars[t,j,p].solution_value\n",
    "                    except:\n",
    "                        try:\n",
    "                            u_variable_value[t][j]={}\n",
    "                            u_variable_value[t][j][p] = u_vars[t,j,p].solution_value\n",
    "                        except:\n",
    "                            try:\n",
    "                                u_variable_value[t]={}\n",
    "                                u_variable_value[t][j]={}\n",
    "                                u_variable_value[t][j][p] = u_vars[t,j,p].solution_value\n",
    "                            except ValueError:\n",
    "                                print(ValueError)\n",
    "    opt_model.clear()\n",
    "\n",
    "    return objective_value,w_variable_value,u_variable_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPLEX_resource_cinsumption_minimization_with_static_variables(network,work_load,life_time,iteration,cyclic_workload,w_variable_value,u_variable_value):\n",
    "    if cyclic_workload ==\"cyclic\":\n",
    "        cyclic_workload=True\n",
    "    else:\n",
    "        cyclic_workload= False\n",
    "    import docplex.mp.model as cpx\n",
    "    opt_model = cpx.Model(name=\"Storage problem model\"+str(iteration))\n",
    "    w_vars = {}\n",
    "    u_vars = {}\n",
    "    #opt_model = Model(name=\"Storage problem model\")\n",
    "#     for t in work_load.T:\n",
    "#         for k in work_load.each_t_requests[t]:\n",
    "#             for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "#     x_vars  = {(t,k,p): opt_model.integer_var(lb=0, ub= 100,\n",
    "#                               name=\"x_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T \n",
    "#                for k in work_load.each_t_requests[t] \n",
    "#                for p in network.each_request_virtual_paths[k]}\n",
    "    \n",
    "#     s_vars  = {(t,k,p): opt_model.integer_var(lb=0, ub= 100,\n",
    "#                               name=\"s_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T \n",
    "#                for k in work_load.each_t_requests[t] \n",
    "#                for p in network.each_request_real_paths[k]}\n",
    "    w_vars  = {(t,k,p): opt_model.continuous_var(lb=0, ub= network.max_edge_capacity,\n",
    "                              name=\"w_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T \n",
    "               for k in work_load.each_t_requests[t] \n",
    "               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]}\n",
    "\n",
    "    u_vars  = {(t,j,p): opt_model.continuous_var(lb=0, ub= network.max_edge_capacity,\n",
    "                                  name=\"u_{0}_{1}_{2}\".format(t,j,p))  for t in work_load.T \n",
    "                   for j in network.storage_pairs for p in network.each_request_real_paths[j]}   \n",
    "\n",
    "    if life_time ==1000:\n",
    "        #inventory evolution constraint\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    \n",
    "                    \n",
    "#                     print(\"2for time %s and storage pair %s and real sub path %s\"%(t,j,p_s))\n",
    "#                     for k in work_load.each_t_requests[t]:\n",
    "#                         print(\"2this is the request \",k)\n",
    "#                         if k!=j:\n",
    "#                             print(\"2which was not equal to storage pair\")\n",
    "#                             for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "#                                 print(\"2this path %s include the sub path %s\"%(p,p_s))\n",
    "\n",
    "                    \n",
    "                    if cyclic_workload:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == u_vars[(t-1)%len(work_load.T),j,p_s]-\n",
    "                        opt_model.sum(w_vars[(t-1)%len(work_load.T),k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                        +opt_model.sum(w_vars[(t-1)%len(work_load.T),j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "                    else:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == u_vars[t-1,j,p_s]-\n",
    "                        opt_model.sum(w_vars[t-1,k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                        +opt_model.sum(w_vars[t-1,j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "    else:\n",
    "        #inventory evolution constraint\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    \n",
    "                    if cyclic_workload:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == -\n",
    "                        opt_model.sum(w_vars[(t-1)%len(work_load.T),k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] \n",
    "                        )\n",
    "                        + opt_model.sum(w_vars[(t-1)%len(work_load.T),j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "                    else:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == -\n",
    "                        opt_model.sum(w_vars[t-1,k,p] *network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] \n",
    "                        )\n",
    "                        + opt_model.sum(w_vars[t-1,j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "\n",
    "    # serving from inventory constraint\n",
    "    for t in work_load.T[1:]:\n",
    "        for j in network.storage_pairs:\n",
    "            \n",
    "            for p_s in network.each_request_real_paths[j]:\n",
    "#                 print(\"for time %s and storage pair %s and real sub path %s\"%(t,j,p_s))\n",
    "#                 for k in work_load.each_t_requests[t]:\n",
    "#                     print(\"this is the request \",k)\n",
    "#                     if k!=j:\n",
    "#                         print(\"which was not equal to storage pair\")\n",
    "#                         for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "#                             print(\"this path %s include the sub path %s\"%(p,p_s))\n",
    "                            \n",
    "                opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]*network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t))\n",
    "                for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] if k in list(network.each_request_virtual_paths_include_subpath.keys()))<=u_vars[t,j,p_s]\n",
    "                                     , ctname=\"inventory_serving_{0}_{1}_{2}\".format(t,j,p_s))  \n",
    " \n",
    "     \n",
    "    # Demand constriant\n",
    "    for t in work_load.T[1:]:\n",
    "        for k in  work_load.each_t_requests[t]:\n",
    "            opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]) >= \n",
    "                    work_load.each_t_each_request_demand[t][k], ctname=\"constraint_{0}_{1}\".format(t,k))\n",
    "    \n",
    "    #Edge constraint\n",
    "    for t in work_load.T:\n",
    "        for edge in network.set_E:\n",
    "            opt_model.add_constraint(\n",
    "                opt_model.sum(w_vars[t,k,p]*network.get_required_purification_EPR_pairs(p,work_load.get_each_request_threshold(k,t)) for k in work_load.each_t_requests[t]\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k] if network.check_path_include_edge(edge,p))\n",
    "                                     \n",
    "                 <= network.each_edge_capacity[edge], ctname=\"edge_capacity_{0}_{1}\".format(t,edge))\n",
    "     \n",
    "    # storage servers capacity constraint\n",
    "    for t in work_load.T:\n",
    "        for s1 in network.storage_nodes:\n",
    "            opt_model.add_constraint(opt_model.sum(u_vars[t,(s1,s2),p] \n",
    "                for s2 in network.storage_nodes if network.check_storage_pair_exist(s1,s2)\n",
    "                for p in network.each_request_real_paths[(s1,s2)])\n",
    "        <= network.get_storage_capacity(s1), ctname=\"storage_capacity_constraint_{0}_{1}\".format(t,s1))\n",
    "    \n",
    "    # constraints for serving from storage at time zero and 1 should be zero\n",
    "    if not cyclic_workload:\n",
    "        for t in [0,1]:\n",
    "            opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                    for k in work_load.each_t_requests[t] for p in network.each_request_virtual_paths[k] \n",
    "                    )<=0, ctname=\"serving_from_inventory_{0}\".format(t))\n",
    "    \n",
    "    # constraints for putting in storage at time zero  should be zero\n",
    "    \"\"\"this is becasue we start the formulation from 1 and not from zero and we have t-1 in our formulation\"\"\"\n",
    "    for t in [0]:\n",
    "        opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                for k in work_load.each_t_requests[t] for p in network.each_request_real_paths[k] \n",
    "                )<=0, ctname=\"storing_in_inventory_{0}\".format(t))   \n",
    "    \n",
    "\n",
    "    # constraint for inventory is zero at time zero \n",
    "    if not cyclic_workload:\n",
    "        for t in [0]:\n",
    "            for j in network.storage_pairs:\n",
    "                 for p_s in network.each_request_real_paths[j]:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] <=0, ctname=\"storage_capacity_constraint_{0}_{1}_{2}\".format(t,j,p_s))\n",
    "    \n",
    "    \"\"\"new constraints for online algorithms to force the variables to be the previous values\"\"\"\n",
    "    \n",
    "    for t in work_load.T:\n",
    "         for k in work_load.each_t_requests[t]:\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                try:\n",
    "                    if w_variable_value[t][k][p]:\n",
    "                        opt_model.add_constraint(w_vars[t,k,p]==w_variable_value[t][k][p], ctname=\"online_alg_w_constraint_{0}_{1}_{2}\".format(t,k,p))\n",
    "                except:\n",
    "                    pass\n",
    "    for t in work_load.T:\n",
    "            for j in network.storage_pairs:\n",
    "                for p in network.each_request_real_paths[j]:\n",
    "                    try:\n",
    "                        if u_variable_value[t][j][p]:\n",
    "                            opt_model.add_constraint(u_vars[t,j,p]==u_variable_value[t][j][p], ctname=\"online_alg_u_constraint_{0}_{1}_{2}\".format(t,k,p))\n",
    "                    except ValueError:\n",
    "                        print(ValueError)\n",
    "    \"\"\"defining an objective, which is a linear expression\"\"\"\n",
    "#     objective = opt_model.sum(1/len(work_load.T[1:])*1/len(work_load.each_t_real_requests[t])*1/work_load.each_t_each_request_demand[t][k]\n",
    "#                               *(w_vars[t,k,p] * network.get_path_length(p)) for t in work_load.T[1:]\n",
    "#                               for k in work_load.each_t_real_requests[t] \n",
    "#                               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]\n",
    "#                               )\n",
    "    objective = opt_model.sum(1/len(work_load.T[1:])*1/len(work_load.each_t_real_requests[t])*1/work_load.each_t_each_request_demand[t][k]\n",
    "                              *(w_vars[t,k,p] * network.get_path_length(p)) for t in work_load.T[1:]\n",
    "                              for k in work_load.each_t_real_requests[t] \n",
    "                              for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]\n",
    "                              )\n",
    "#     opt_model.minimize(1/len(work_load.T[1:])*\n",
    "#                     1/opt_model.sum(len(work_load.each_t_real_requests[t]) for t in work_load.T[1:])*\n",
    "#                     (opt_model.sum(w_vars[t,k,p] * network.get_path_length(p)/work_load.each_t_each_request_demand[t][k] for t in work_load.T[1:]\n",
    "#                               for k in work_load.each_t_real_requests[t] \n",
    "#                               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]))\n",
    "#                                 )\n",
    "    \n",
    "    # for maximization\n",
    "    opt_model.minimize(objective)\n",
    "    \n",
    "#     opt_model.solve()\n",
    "    opt_model.print_information()\n",
    "    #try:\n",
    "    opt_model.solve()\n",
    "\n",
    "\n",
    "    \n",
    "    print('docplex.mp.solution',opt_model.solution)\n",
    "    objective_value = -1\n",
    "    try:\n",
    "        if opt_model.solution:\n",
    "            objective_value =opt_model.solution.get_objective_value()\n",
    "    except ValueError:\n",
    "        print(ValueError)\n",
    "        #pass\n",
    "    \n",
    "    opt_model.clear()\n",
    "\n",
    "    return objective_value,w_variable_value,u_variable_value\n",
    "\n",
    "def get_each_t_user_pairs(topology):\n",
    "    each_t_user_pairs = {0: [(19, 24), (0, 18), (11, 23)], \n",
    "                     1: [(19, 24), (0, 18), (11, 23)], \n",
    "                     2: [(19, 24), (0, 18), (11, 23)], \n",
    "                     3: [(19, 24), (0, 18), (11, 23)], \n",
    "                     4: [(19, 24), (0, 18), (11, 23)], \n",
    "                     5: [(19, 24), (0, 18), (11, 23)], \n",
    "                     6: [(19, 24), (0, 18), (11, 23)], \n",
    "                     7: [(19, 24), (0, 18), (11, 23)], \n",
    "                     8: [(19, 24), (0, 18), (11, 23)], \n",
    "                     9: [(19, 24), (0, 18), (11, 23)], \n",
    "                     10: [(19, 24), (0, 18), (11, 23)], \n",
    "                     11: [(19, 24), (0, 18), (11, 23)], \n",
    "                     12: [(19, 24), (0, 18), (11, 23)], \n",
    "                     13: [(19, 24), (0, 18), (11, 23)], \n",
    "                     14: [(19, 24), (0, 18), (11, 23)]}\n",
    "    return each_t_user_pairs\n",
    "def predict_demands(topology,each_t_user_pairs,algorithm_type,window_size,given_exp_id,given_time):\n",
    "    real_demands_file_path = \"user_pair_demands\"\n",
    "    if algorithm_type==\"offline\":\n",
    "        for user_pair_index in range(len(each_t_user_pairs[time])):\n",
    "            with open(results_file_path+str(user_pair_index)+\".csv\", \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\",\")\n",
    "            for line in (reader):\n",
    "                time = line[]\n",
    "                pair_indx = int(line[])\n",
    "                experiment_id = int(line[])\n",
    "                if user_pair_indx!=pair_indx:\n",
    "                    print(\"ERROR\")\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                else:\n",
    "                    if experiment_id==i:\n",
    "                        user_pair = each_t_user_pairs[time][user_pair_indx]\n",
    "                        demand = float(line)\n",
    "                        each_t_each_user_demands[time][user_pair] = demand\n",
    "#         each_t_each_user_demands = {0: {(19, 24): 0, (0, 18): 0, (11, 23): 0}, \n",
    "#                                 1: {(19, 24): 1, (0, 18): 151.2292026424477, (11, 23): 1},\n",
    "#                                 2: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 3: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 4: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 5: {(19, 24): 7.767696075282146, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 6: {(19, 24): 25.01422142522252, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 7: {(19, 24): 43.25445281798754, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 8: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 9: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 10: {(19, 24): 1, (0, 18): 10.50872851623511, (11, 23): 1}, \n",
    "#                                 11: {(19, 24): 40.88185163197947, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 12: {(19, 24): 34.87105588563392, (0, 18): 1, (11, 23): 1}, \n",
    "#                                 13: {(19, 24): 1, (0, 18): 1, (11, 23): 1},\n",
    "#                                 14: {(19, 24): 1, (0, 18): 1, (11, 23): 23.221812051461683}}\n",
    "    elif algorithm_type ==\"RHC\" or algorithm_type ==\"AFHC\":\n",
    "        \n",
    "        for user_pair_index in range(len(each_t_user_pairs[time])):\n",
    "            with open(results_file_path+str(user_pair_index)+\".csv\", \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\",\")\n",
    "            for line in (reader):\n",
    "                time = line[]\n",
    "                pair_indx = int(line[])\n",
    "                experiment_id = int(line[])\n",
    "                starting_time = int(line[])\n",
    "                \n",
    "                if experiment_id==i and starting_time ==given_time:\n",
    "                    user_pair = each_t_user_pairs[time][user_pair_indx]\n",
    "                    demand = float(line)\n",
    "                    each_t_each_user_demands[time][user_pair] = demand\n",
    "\n",
    "        \n",
    "        \n",
    "        each_t_each_user_demands = {0: {(19, 24): 0, (0, 18): 0, (11, 23): 0}, \n",
    "                                1: {(19, 24): 1, (0, 18): 151.2292026424477, (11, 23): 1},\n",
    "                                2: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                3: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                4: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                5: {(19, 24): 7.767696075282146, (0, 18): 1, (11, 23): 1}, \n",
    "                                6: {(19, 24): 25.01422142522252, (0, 18): 1, (11, 23): 1}, \n",
    "                                7: {(19, 24): 43.25445281798754, (0, 18): 1, (11, 23): 1}, \n",
    "                                8: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                9: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                10: {(19, 24): 1, (0, 18): 10.50872851623511, (11, 23): 1}, \n",
    "                                11: {(19, 24): 40.88185163197947, (0, 18): 1, (11, 23): 1}, \n",
    "                                12: {(19, 24): 34.87105588563392, (0, 18): 1, (11, 23): 1}, \n",
    "                                13: {(19, 24): 1, (0, 18): 1, (11, 23): 1},\n",
    "                                14: {(19, 24): 1, (0, 18): 1, (11, 23): 23.221812051461683}}\n",
    "    elif algorithm_type ==\"AFHC\":\n",
    "        each_t_each_user_demands = {0: {(19, 24): 0, (0, 18): 0, (11, 23): 0}, \n",
    "                                1: {(19, 24): 1, (0, 18): 151.2292026424477, (11, 23): 1},\n",
    "                                2: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                3: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                4: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                5: {(19, 24): 7.767696075282146, (0, 18): 1, (11, 23): 1}, \n",
    "                                6: {(19, 24): 25.01422142522252, (0, 18): 1, (11, 23): 1}, \n",
    "                                7: {(19, 24): 43.25445281798754, (0, 18): 1, (11, 23): 1}, \n",
    "                                8: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                9: {(19, 24): 1, (0, 18): 1, (11, 23): 1}, \n",
    "                                10: {(19, 24): 1, (0, 18): 10.50872851623511, (11, 23): 1}, \n",
    "                                11: {(19, 24): 40.88185163197947, (0, 18): 1, (11, 23): 1}, \n",
    "                                12: {(19, 24): 34.87105588563392, (0, 18): 1, (11, 23): 1}, \n",
    "                                13: {(19, 24): 1, (0, 18): 1, (11, 23): 1},\n",
    "                                14: {(19, 24): 1, (0, 18): 1, (11, 23): 23.221812051461683}}\n",
    "        \n",
    "\n",
    "    return each_t_each_user_demands\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (554742027.py, line 210)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/23/q7nr2mkd60z1t1fs92cd0rd40000gp/T/ipykernel_32811/554742027.py\"\u001b[0;36m, line \u001b[0;32m210\u001b[0m\n\u001b[0;31m    def predict_demands(algorithm_type,window_size,i,0):\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def experiment(each_network_topology_file,spike_means,num_spikes,experiment_repeat,storage_node_selection_schemes,fidelity_threshold_ranges,cyclic_workload):\n",
    "    for spike_mean in spike_means:\n",
    "        for network_topology,file_path in each_network_topology_file.items():\n",
    "            distance_between_users = 2\n",
    "            number_of_user_pairs =3\n",
    "            number_of_time_slots = 12\n",
    "            network = Network(file_path)\n",
    "            network.get_user_pairs_online(number_of_user_pairs,distance_between_users,number_of_time_slots)\n",
    "                    \n",
    "            for window_size in prediction_window_sizes:\n",
    "                for i in range(experiment_repeat):\n",
    "                    \n",
    "                    life_time = 1000\n",
    "                    work_load = Work_load(number_of_time_slots,\"time_demands_file.csv\")\n",
    "                    \"\"\"we set the demands for each user pair\"\"\"\n",
    "\n",
    "                    fidelity_threshold_range =0.7\n",
    "                    network.set_user_pair_fidelity_threshold(fidelity_threshold_range)\n",
    "\n",
    "                    storage_node_selection_scheme =\"Degree\"\n",
    "\n",
    "                    objective_values = []\n",
    "                    selected_storage_nodes = []\n",
    "                    selected_storage_pairs = []\n",
    "\n",
    "                    num_paths= 3\n",
    "                    number_of_storages=5\n",
    "                    network.each_request_real_paths = {}\n",
    "                    network.reset_pair_paths()\n",
    "                    \"\"\"with new storage pairs, we will check the solution for each number of paths(real and virtual)\"\"\"\n",
    "                    work_load.reset_variables()\n",
    "                    pairs = []\n",
    "                    #print(\"network.each_t_user_pairs\",network.each_t_user_pairs)\n",
    "                    for t,user_pairs in network.each_t_user_pairs.items():            \n",
    "                        for user_pair in user_pairs:\n",
    "                            if user_pair not in pairs:\n",
    "                                pairs.append(user_pair)\n",
    "                    network.get_each_user_pair_real_paths(pairs)\n",
    "                    import pdb\n",
    "                    #pdb.set_trace()\n",
    "                    path_counter_id = 0\n",
    "\n",
    "                    \"\"\"select and add new storage pairs\"\"\"\n",
    "                    network.get_new_storage_pairs(number_of_storages,storage_node_selection_scheme)\n",
    "                    print(\"these are our storage pairs\",network.storage_pairs)\n",
    "                    print(\"network.each_t_user_pairs\",network.each_t_user_pairs)\n",
    "                    work_load.set_each_time_requests(network.each_t_user_pairs,network.storage_pairs)\n",
    "                    work_load.set_each_time_real_requests(network.each_t_user_pairs)\n",
    "\n",
    "                    #print(\"network.storage_pairs\",network.storage_pairs)\n",
    "                    #import pdb\n",
    "                    #pdb.set_trace()\n",
    "                    network.get_each_user_pair_real_paths(network.storage_pairs)\n",
    "                    if number_of_storages==1:\n",
    "                        number_of_storages = 2\n",
    "\n",
    "                    \"\"\"first we add the real paths between storage pairs\"\"\"\n",
    "\n",
    "                    print(\"for spike mean %s topology %s fidelity range %s iteration %s storage %s and path number %s\"%\n",
    "                          (spike_mean,network_topology,fidelity_threshold_range,i,number_of_storages,num_paths))\n",
    "                    for storage_pair in network.storage_pairs:\n",
    "                #                         print(\"going to get real paths between storage pair \",storage_pair)\n",
    "                        paths = network.get_real_path(storage_pair,num_paths)\n",
    "                        #print(\"got paths\",paths)\n",
    "                        for path in paths:\n",
    "                            network.set_each_path_length(path_counter_id,path)\n",
    "                            network.set_of_paths[path_counter_id] = path\n",
    "                            network.each_path_path_id[tuple(path)] = path_counter_id\n",
    "                            try:\n",
    "                                network.each_request_real_paths[storage_pair].append(path_counter_id)\n",
    "                            except:\n",
    "                                network.each_request_real_paths[storage_pair]=[path_counter_id]\n",
    "                            try:\n",
    "                                network.each_storage_real_paths[storage_pair].append(path)\n",
    "                            except:\n",
    "                                network.each_storage_real_paths[storage_pair]=[path]\n",
    "                #                             print(\"*** we added path %s for storage %s and used path_counter_id %s\"%(path,storage_pair,path_counter_id))\n",
    "                            path_counter_id+=1\n",
    "                #                     for path,path_ID in network.each_path_path_id.items():\n",
    "                #                         print(\"each path %s has path ID %s \"%(path,path_ID))\n",
    "\n",
    "                    across_all_time_slots_pairs = []\n",
    "                    for t,user_pairs in network.each_t_user_pairs.items():\n",
    "                        for user_pair in user_pairs:\n",
    "                            if user_pair not in across_all_time_slots_pairs:\n",
    "                                across_all_time_slots_pairs.append(user_pair)\n",
    "                    all_sub_paths = []\n",
    "                    for user_pair in across_all_time_slots_pairs:\n",
    "                        paths = network.get_real_path(user_pair,num_paths)\n",
    "                        #print(\"we got real paths for user pair\",user_pair,paths)\n",
    "                        for path in paths:\n",
    "                            network.set_of_paths[path_counter_id] = path\n",
    "                            network.set_each_path_length(path_counter_id,path)\n",
    "                            network.each_path_path_id[tuple(path)] = path_counter_id\n",
    "                            try:\n",
    "                                network.each_request_real_paths[user_pair].append(path_counter_id)\n",
    "                            except:\n",
    "                                network.each_request_real_paths[user_pair]=[path_counter_id]\n",
    "                            #print(\"*** we used path_counter_id\",path_counter_id)\n",
    "                            path_counter_id+=1\n",
    "                #             print(\"for user pair  we got real paths and it is\",user_pair)\n",
    "                #                         print(\"network.each_request_real_paths\",network.each_request_real_paths)\n",
    "                #                         print(\"network.set_of_paths\",network.set_of_paths)\n",
    "                        import pdb\n",
    "                        #pdb.set_trace()\n",
    "                        for storage_pair in network.storage_pairs:\n",
    "                            \"\"\"add one new path to the previous paths\"\"\"\n",
    "\n",
    "                            for real_sub_path in network.each_storage_real_paths[storage_pair]:\n",
    "                                #for edge in real_sub_path:\n",
    "                                    #network.g.remove_edge(edge[0],edge[1])\n",
    "                                #network.g.add_edge(storage_pair[0],storage_pair[1],weight=0)\n",
    "                #                                 print(\"we are going to add a virtual path for user pair %s that includes %s\"%(user_pair,real_sub_path))\n",
    "                                paths = network.get_paths_to_connect_users_to_storage(user_pair,real_sub_path,num_paths)\n",
    "\n",
    "                                this_sub_path_id = network.each_path_path_id[tuple(real_sub_path)]\n",
    "                                #print(paths)\n",
    "\n",
    "                                for path in paths:\n",
    "                                    path = network.remove_storage_pair_real_path_from_path(real_sub_path,path)\n",
    "                                    network.set_each_path_length(path_counter_id,path)\n",
    "                                    \"\"\"we remove the sub path that is connecting two storage pairs \n",
    "                                    from the path because we do not want to check the edge capacity for the edges of this subpath\"\"\"\n",
    "                #                                     print(\"we set length %s for path %s having sub path %s with ID %s\"%(len(path),path,real_sub_path,this_sub_path_id))\n",
    "                                    try:\n",
    "                                        network.each_request_virtual_paths_include_subpath[user_pair][this_sub_path_id].append(path_counter_id)\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            network.each_request_virtual_paths_include_subpath[user_pair][this_sub_path_id]=[path_counter_id]\n",
    "                                        except:\n",
    "                                            network.each_request_virtual_paths_include_subpath[user_pair]={}\n",
    "                                            network.each_request_virtual_paths_include_subpath[user_pair][this_sub_path_id]=[path_counter_id]\n",
    "                #                                     if number_of_storages>0:\n",
    "                #                                         print(\"paths_include_subpath: we added path id %s  for sub path %s for user pairs %s storage pair %s\"%(path_counter_id,this_sub_path_id,user_pair,storage_pair))\n",
    "                                        #time.sleep(5)\n",
    "                                    if this_sub_path_id not in all_sub_paths:\n",
    "                                        all_sub_paths.append(this_sub_path_id)\n",
    "\n",
    "                                    #print(\"and after removing sub path we have \",path,real_sub_path,len(path))\n",
    "                                    network.set_of_paths[path_counter_id] = path\n",
    "                                    try:\n",
    "                                        network.each_request_virtual_paths[user_pair].append(path_counter_id)\n",
    "                                    except:\n",
    "                                        network.each_request_virtual_paths[user_pair]=[path_counter_id]\n",
    "                #                                     if number_of_storages>0:\n",
    "                #                                         print(\"***Virtual paths:  we added virtual path %s and subpath was %s to user pair %s storage pair %s %s\"%(path_counter_id,this_sub_path_id,user_pair,storage_pair,\"\\n\"))\n",
    "                                        #time.sleep(1)\n",
    "                                    path_counter_id+=1\n",
    "\n",
    "\n",
    "                                for pair in network.storage_pairs:\n",
    "                                    network.each_request_virtual_paths[pair]=[]\n",
    "\n",
    "\n",
    "                    if number_of_storages==0:\n",
    "                        for t,pairs in network.each_t_user_pairs.items():\n",
    "                            for pair in pairs:\n",
    "                                network.each_request_virtual_paths[pair]=[]\n",
    "                                for j in network.storage_pairs:\n",
    "                                    for sub_path_id in all_sub_paths:\n",
    "                                        network.each_request_virtual_paths_include_subpath[pair][sub_path_id]={}\n",
    "                    for j in network.storage_pairs:\n",
    "                        for sub_path_id in all_sub_paths:\n",
    "                            try:\n",
    "                                network.each_request_virtual_paths_include_subpath[j][sub_path_id] = []\n",
    "                            except:\n",
    "                                network.each_request_virtual_paths_include_subpath[j]={}\n",
    "                                network.each_request_virtual_paths_include_subpath[j][sub_path_id] = []\n",
    "                    \n",
    "                    for t in range(number_of_time_slots):\n",
    "                        for k in work_load.each_t_requests[t]:\n",
    "                            try:\n",
    "                                if k in list(network.each_request_virtual_paths_include_subpath.keys()):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    network.each_request_virtual_paths_include_subpath[k]= {}\n",
    "                            except:\n",
    "                                network.each_request_virtual_paths_include_subpath[k]= {}\n",
    "\n",
    "                    \"\"\"we set the capacity of each storage node\"\"\"\n",
    "\n",
    "                    network.set_storage_capacity()\n",
    "\n",
    "                    \"\"\"we add new storage pairs as our user pairs and set the demand for them zero\"\"\"\n",
    "\n",
    "                    work_load.set_storage_pairs_as_user_pairs(network.storage_pairs)\n",
    "#                     print(\"work_load.each_t_requests\",work_load.each_t_requests)\n",
    "#                     print(\"work_load.each_t_each_request_demand\",work_load.each_t_each_request_demand)\n",
    "\n",
    "                    \"\"\"we set the fidelity threshold of each new storage pair as a user request\"\"\"\n",
    "                    network.set_each_path_basic_fidelity()\n",
    "                    work_load.set_threshold_fidelity_for_request_pairs(network.each_t_user_pairs,network.storage_pairs,network.each_user_request_fidelity_threshold)\n",
    "                    #print(\"setting required EPR pairs\")\n",
    "                    \"\"\"we set the required EPR pairs to achieve each request threshold fidelity\"\"\"\n",
    "                    network.set_required_EPR_pairs_for_path_fidelity_threshold()\n",
    "\n",
    "\n",
    "\n",
    "                    \"\"\"we print all variables to check the variables and values\"\"\"\n",
    "\n",
    "                    \"\"\"solve the optimization\"\"\"\n",
    "                    online_algorithms = [\"offline\",\"RHC\",\"AFHC\",\"pre avg\"]\n",
    "                    #print(\"solving offline algorithm\")\n",
    "                    each_t_each_user_pair_real_demands = predict_demands(network_topology,network.each_t_user_pairs,\"offline\",\n",
    "                                                                         window_size,i,0)\n",
    "\n",
    "                    work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_real_demands)\n",
    "                    \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                    work_load.check_demands_per_each_time(network.each_t_user_pairs)  \n",
    "                    objective_value= -1\n",
    "#                     for time in range(number_of_time_slots):\n",
    "#                         for user_pair in network.each_t_user_pairs[t]:\n",
    "#                             work_load.each_t_each_request_demand[time][user_pair] = each_t_each_user_pair_real_demands[t][user_pair]\n",
    "                    optimal_offline,_,_ = CPLEX_resource_cinsumption_minimization(network,work_load,life_time,i,cyclic_workload,window_size)\n",
    "                    print(\"objective value for optimal offline is \",optimal_offline)\n",
    "                    if optimal_offline>0:\n",
    "                        print(\"solving RHC algorithm\")\n",
    "                        \"\"\"implementing RHC\"\"\"\n",
    "                        all_times_feasibility_flag = True\n",
    "                        w_variable_value = {}\n",
    "                        u_variable_value = {}\n",
    "                        for time in range(number_of_time_slots):\n",
    "                            if all_times_feasibility_flag:\n",
    "                                each_t_each_user_pair_predicted_demands = predict_demands(network_topology,network.each_t_user_pairs,\"RHC\",window_size,i,time)\n",
    "                                \n",
    "                                work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_predicted_demands)\n",
    "                                \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                                work_load.check_demands_per_each_time(network.each_t_user_pairs)  \n",
    "\n",
    "                                RHC_based_alg_solution = -1\n",
    "                                try:\n",
    "                                    RHC_based_alg_solution,returned_w_variable_values,returned_u_variable_values = CPLEX_resource_cinsumption_minimization(network,work_load,life_time,i,cyclic_workload,window_size)\n",
    "                                    if RHC_based_alg_solution<0:\n",
    "                                        print(\"THIS IS AN ERROR\")\n",
    "                                        import pdb\n",
    "                                        pdb.set_trace()\n",
    "                                    \"we get the first solution\"\n",
    "                                    if RHC_based_alg_solution>=0:\n",
    "                                        for t in range(number_of_time_slots):\n",
    "                                            if t == time:\n",
    "                                                 for k in work_load.each_t_requests[t]:\n",
    "                                                    for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                                                        try:\n",
    "                                                            w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                w_variable_value[t][k]={}\n",
    "                                                                w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    w_variable_value[t]={}\n",
    "                                                                    w_variable_value[t][k]={}\n",
    "                                                                    w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                                except ValueError:\n",
    "                                                                    print(ValueError)\n",
    "\n",
    "                                        for t in range(number_of_time_slots):\n",
    "                                            if t == time:\n",
    "                                                for j in network.storage_pairs:\n",
    "                                                    for p in network.each_request_real_paths[j]: \n",
    "                                                        try:\n",
    "                                                            u_variable_value[t][j][p] = returned_u_variable_values[t,j,p]\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                u_variable_value[t][j]={}\n",
    "                                                                u_variable_value[t][j][p] = returned_u_variable_values[t,j,p]\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    u_variable_value[t]={}\n",
    "                                                                    u_variable_value[t][j]={}\n",
    "                                                                    u_variable_value[t][j][p] = returned_u_variable_values[t,j,p]\n",
    "                                                                except ValueError:\n",
    "                                                                    print(ValueError)\n",
    "                                except:\n",
    "                                    all_times_feasibility_flag = False\n",
    "                                    variable_values = {}\n",
    "\n",
    "                        if all_times_feasibility_flag:\n",
    "                            \n",
    "                            work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_real_demands)\n",
    "                            \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                            work_load.check_demands_per_each_time(network.each_t_user_pairs)  \n",
    "                            prediction_based_solution_satisfy_real_demands = -1\n",
    "                            try:\n",
    "                                prediction_based_solution_satisfy_real_demands,_,_ = CPLEX_resource_cinsumption_minimization_with_static_variables(network,work_load,life_time,i,cyclic_workload,w_variable_value, u_variable_value)\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "                            if prediction_based_solution_satisfy_real_demands>=0:\n",
    "                                print(\"RHC has a solution!\")\n",
    "                                import pdb\n",
    "                                pdb.set_trace()\n",
    "                                RHC_alg_solution = 4\n",
    "                            else:\n",
    "                                RHC_alg_solution = -1\n",
    "                            if RHC_alg_solution>0:\n",
    "                                print(\"for pridection window %s experiment num %s we got satisfaction for RHC\"%(window_size,i)) \n",
    "                            else:\n",
    "                                print(\"for pridection window %s experiment num %s we did not get satisfaction for RHC\"%(window_size,i)) \n",
    "                        print(\"implementing AFHC\")\n",
    "                        \"\"\"implementing AFHC\"\"\"\n",
    "                        w_variable_value = {}\n",
    "                        u_variable_value = {}\n",
    "                        all_times_feasibility_flag = True\n",
    "                        for t in range(number_of_time_slots):\n",
    "                            if all_times_feasibility_flag:\n",
    "                                each_t_each_user_pair_predicted_demands = predict_demands(network_topology,network.each_t_user_pairs,\"AFHC\",window_size,i,time)\n",
    "\n",
    "                                for time in range(number_of_time_slots):\n",
    "                                    for user_pair in network.each_t_user_pairs[t]:\n",
    "                                        work_load.each_t_each_request_demand[time][user_pair] = each_t_each_user_pair_predicted_demands[t][user_pair]\n",
    "                                work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_predicted_demands)\n",
    "                                \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                                work_load.check_demands_per_each_time(network.each_t_user_pairs) \n",
    "                                AFHC_based_alg_solution = -1\n",
    "                                try:\n",
    "                                    AFHC_based_alg_solution,returned_w_variable_values,returned_u_variable_values = CPLEX_resource_cinsumption_minimization(network,work_load,life_time,i,cyclic_workload,window_size)\n",
    "\n",
    "                                    if RHC_based_alg_solution>=0:\n",
    "                                            for t in work_load.T:\n",
    "                                                 for k in work_load.each_t_requests[t]:\n",
    "                                                    for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                                                        try:\n",
    "                                                            w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                w_variable_value[t][k]={}\n",
    "                                                                w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    w_variable_value[t]={}\n",
    "                                                                    w_variable_value[t][k]={}\n",
    "                                                                    w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                                except ValueError:\n",
    "                                                                    print(ValueError)\n",
    "\n",
    "                                            for t in work_load.T:\n",
    "                                                for j in network.storage_pairs:\n",
    "                                                    for p in network.each_request_real_paths[j]: \n",
    "                                                        try:\n",
    "                                                            u_variable_value[t][j][p].append(returned_u_variable_values[t,j,p])\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                u_variable_value[t][j]={}\n",
    "                                                                u_variable_value[t][j][p] = [returned_u_variable_values[t,j,p]]\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    u_variable_value[t]={}\n",
    "                                                                    u_variable_value[t][j]={}\n",
    "                                                                    u_variable_value[t][j][p] = [returned_u_variable_values[t,j,p]]\n",
    "                                                                except ValueError:\n",
    "                                                                    print(ValueError)\n",
    "                                except:\n",
    "                                    all_times_feasibility_flag = False\n",
    "                                    variable_values = {}\n",
    "                                \n",
    "                        if all_times_feasibility_flag:\n",
    "                            for t in work_load.T:\n",
    "                                 for k in work_load.each_t_requests[t]:\n",
    "                                    for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                                        try:\n",
    "                                            w_variable_value[t][k][p] = sum(w_variable_value[t][k][p])/len(w_variable_value[t][k][p])\n",
    "                                        except:\n",
    "                                            pass\n",
    "                            for t in work_load.T:\n",
    "                                for j in network.storage_pairs:\n",
    "                                    for p in network.each_request_real_paths[j]: \n",
    "                                        try:\n",
    "                                            u_variable_value[t][j][p] = sum(u_variable_value[t][j][p])/len(u_variable_value[t][j][p])\n",
    "                                        except:\n",
    "                                            pass\n",
    "                            if all_times_feasibility_flag:\n",
    "                                for time in range(number_of_time_slots):\n",
    "                                    for user_pair in network.each_t_user_pairs[t]:\n",
    "                                        work_load.each_t_each_request_demand[time][user_pair] = each_t_each_user_pair_real_demands[t][user_pair]\n",
    "\n",
    "                                work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_real_demands)\n",
    "                                \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                                work_load.check_demands_per_each_time(network.each_t_user_pairs)\n",
    "\n",
    "                                prediction_based_solution_satisfy_real_demands = -1\n",
    "                                try:\n",
    "                                    prediction_based_solution_satisfy_real_demands,_,_ = CPLEX_resource_cinsumption_minimization_with_static_variables(network,work_load,life_time,i,cyclic_workload,w_variable_value, u_variable_value)\n",
    "                                except:\n",
    "                                    pass\n",
    "                                if prediction_based_solution_satisfy_real_demands>=0:\n",
    "                                    AFHC_alg_solution = 4\n",
    "                                else:\n",
    "                                    AFHC_alg_solution = -1\n",
    "                            if AFHC_alg_solution>0:\n",
    "                                print(\"for pridection window %s experiment num %s we got satisfaction for AFHC\"%(window_size,i)) \n",
    "                            else:\n",
    "                                print(\"for pridection window %s experiment num %s we did not get satisfaction for AFHC\"%(window_size,i)) \n",
    "\n",
    "                        \"\"\"implementing pre-avg scheme\"\"\"\n",
    "                        print(\"solving pre-avg algorithm\")\n",
    "                        \"\"\"implementing pre-avg\"\"\"\n",
    "                        all_times_feasibility_flag = True\n",
    "                        w_variable_value = {}\n",
    "                        u_variable_value = {}\n",
    "                        for time in range(number_of_time_slots):\n",
    "                            if all_times_feasibility_flag:\n",
    "                                each_t_each_user_pair_predicted_demands = predict_demands(network_topology,network.each_t_user_pairs,\"pre-avg\",window_size,i,time)\n",
    "                                \n",
    "                                work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_predicted_demands)\n",
    "                                \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                                work_load.check_demands_per_each_time(network.each_t_user_pairs)  \n",
    "\n",
    "                                AVG_based_alg_solution = -1\n",
    "                                try:\n",
    "                                    AVG_based_alg_solution,returned_w_variable_values,returned_u_variable_values = CPLEX_resource_cinsumption_minimization(network,work_load,life_time,i,cyclic_workload,window_size)\n",
    "                                    if AVG_based_alg_solution<0:\n",
    "                                        print(\"THIS IS AN ERROR\")\n",
    "                                        import pdb\n",
    "                                        #pdb.set_trace()\n",
    "                                    \"we get the first solution\"\n",
    "                                    if AVG_based_alg_solution>=0:\n",
    "                                        for t in range(number_of_time_slots):\n",
    "                                            if t == time:\n",
    "                                                 for k in work_load.each_t_requests[t]:\n",
    "                                                    for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                                                        try:\n",
    "                                                            w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                w_variable_value[t][k]={}\n",
    "                                                                w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    w_variable_value[t]={}\n",
    "                                                                    w_variable_value[t][k]={}\n",
    "                                                                    w_variable_value[t][k][p] = returned_w_variable_values[t,k,p]\n",
    "                                                                except ValueError:\n",
    "                                                                    print(ValueError)\n",
    "\n",
    "                                        for t in range(number_of_time_slots):\n",
    "                                            if t == time:\n",
    "                                                for j in network.storage_pairs:\n",
    "                                                    for p in network.each_request_real_paths[j]: \n",
    "                                                        try:\n",
    "                                                            u_variable_value[t][j][p] = returned_u_variable_values[t,j,p]\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                u_variable_value[t][j]={}\n",
    "                                                                u_variable_value[t][j][p] = returned_u_variable_values[t,j,p]\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    u_variable_value[t]={}\n",
    "                                                                    u_variable_value[t][j]={}\n",
    "                                                                    u_variable_value[t][j][p] = returned_u_variable_values[t,j,p]\n",
    "                                                                except ValueError:\n",
    "                                                                    print(ValueError)\n",
    "                                except:\n",
    "                                    all_times_feasibility_flag = False\n",
    "                                    variable_values = {}\n",
    "\n",
    "                        if all_times_feasibility_flag:\n",
    "                            \n",
    "                            work_load.set_each_user_pair_demands_online(number_of_time_slots,each_t_each_user_pair_real_demands)\n",
    "                            \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                            work_load.check_demands_per_each_time(network.each_t_user_pairs)  \n",
    "                            prediction_based_solution_satisfy_real_demands = -1\n",
    "                            try:\n",
    "                                prediction_based_solution_satisfy_real_demands,_,_ = CPLEX_resource_cinsumption_minimization_with_static_variables(network,work_load,life_time,i,cyclic_workload,w_variable_value, u_variable_value)\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "                            if prediction_based_solution_satisfy_real_demands>=0:\n",
    "                                print(\"AVG has a solution!\")\n",
    "                                import pdb\n",
    "                                pdb.set_trace()\n",
    "                                AVG_alg_solution = 4\n",
    "                            else:\n",
    "                                AVG_alg_solution = -1\n",
    "                            if AVG_alg_solution>0:\n",
    "                                print(\"for pridection window %s experiment num %s we got satisfaction for AVG\"%(window_size,i)) \n",
    "                            else:\n",
    "                                print(\"for pridection window %s experiment num %s we did not get satisfaction for AVG\"%(window_size,i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_repeat =50\n",
    "spike_means = [80,320,300,200,250,280,340]\n",
    "prediction_window_sizes = [2,4,6,8,10,12,14]\n",
    "num_spikes = 2\n",
    "topology_set = sys.argv[1]\n",
    "storage_node_selection_schemes=[\"Degree\",\"Random\"]\n",
    "cyclic_workload = \"circle\"\n",
    "\n",
    "fidelity_threshold_ranges = [0.65,0.7,0.75,0.9,0.8,0.85,0.95,0.98,1.0]\n",
    "each_network_topology_file = {}\n",
    "if topology_set ==\"real\":\n",
    "    each_network_topology_file = {\"ATT\":'data/ATT_topology_file',\"Abilene\":'data/abilene',\"SURFnet\":'data/Surfnet',\"IBM\":'data/IBM'}\n",
    "# each_network_topology_file = {\"SURFnet\":'data/Surfnet',\"IBM\":'data/IBM'}\n",
    "\n",
    "# each_network_topology_file = {\"random_barabasi_albert2_0\":\"data/random_barabasi_albert2_0.txt\"}\n",
    "# for i in range(1,4):\n",
    "#     each_network_topology_file[\"random_erdos_renyi_\"+str(i)]= \"data/random_erdos_renyi_\"+str(i)+\".txt\"\n",
    "#     each_network_topology_file[\"random_barabasi_albert2_\"+str(i)]= \"data/random_barabasi_albert2_\"+str(i)+\".txt\"\n",
    "elif topology_set ==\"random1\":\n",
    "    for i in [2,4,6]:\n",
    "        each_network_topology_file[\"random_erdos_renyi2_\"+str(i)]= \"data/random_erdos_renyi2_\"+str(i)+\".txt\"\n",
    "        each_network_topology_file[\"random_barabasi_albert2_\"+str(i)]= \"data/random_barabasi_albert2_\"+str(i)+\".txt\"\n",
    "# each_network_topology_file = {\"Testing_topology\":'data/test_topology'}\n",
    "elif topology_set ==\"random2\":\n",
    "    for i in range(1,4):\n",
    "        each_network_topology_file[\"random_erdos_renyi_\"+str(i)]= \"data/random_erdos_renyi_\"+str(i)+\".txt\"\n",
    "        each_network_topology_file[\"random_barabasi_albert_\"+str(i)]= \"data/random_barabasi_albert_\"+str(i)+\".txt\"\n",
    "# import networkx as nx\n",
    "# all_diameters = []\n",
    "# for topology,file in each_network_topology_file.items():\n",
    "#     print(\"for topology\",topology)\n",
    "    \n",
    "#     g = nx.Graph()\n",
    "#     f = open(file, 'r')\n",
    "#     header = f.readline()\n",
    "#     for line in f:\n",
    "#         line = line.strip()\n",
    "#         link = line.split('\\t')\n",
    "#         i, s, d, c = link\n",
    "#         g.add_edge(int(s),int(d),weight=1)\n",
    "#     f.close()  \n",
    "#     all_diameters.append(nx.diameter(g))\n",
    "    \n",
    "experiment(each_network_topology_file,spike_means,num_spikes,experiment_repeat,storage_node_selection_schemes,fidelity_threshold_ranges,cyclic_workload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# for number_of_storages in range(0,2):\n",
    "#     print(number_of_storages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Network(object):\n",
    "#     def __init__(self, config, data_dir='./data/'):\n",
    "#         #self.topology_file = data_dir + config.topology_file\n",
    "#         self.topology_file = 'data'\n",
    "#         self.set_E = [(1,2),(2,3),(3,4),(4,5),(2,7),(7,8),(8,9),(9,5),(5,6),(2,10),(10,11),(11,12),(12,5)]\n",
    "#         self.each_edge_capacity = {(1,2):20,(2,3):20,(3,4):20,(4,5):20,(2,7):20,(7,8):20,(8,9):20,\n",
    "#                                    (9,5):20,(5,6):20,(2,10):20,(10,11):20,(11,12):20,(12,5):20}\n",
    "        \n",
    "#         self.max_edge_capacity = 10\n",
    "#         self.set_of_paths = {0:[(1,2),(2,10),(10,11),(11,12),(12,5),(5,6)],\n",
    "#                              1:[(2,3),(3,4),(4,5)],\n",
    "#                             2:[(2,7),(7,8),(8,9),(9,5)],\n",
    "#                              3:[(10,11),(11,12)],\n",
    "#                             4:[(1,2),(2,5),(5,6)],5:[(1,2),(2,5),(5,6)],\n",
    "#                             6:[(2,10),(10,12),(12,5)],\n",
    "#                             7:[(1,2),(2,10),(10,12),(12,5),(5,6)]}\n",
    "#         self.each_path_basic_fidelity = {0:0.7,\n",
    "#                              1:0.8,\n",
    "#                             2:0.75,\n",
    "#                              3:0.9,\n",
    "#                             4:0.7,\n",
    "#                             5:0.6,\n",
    "#                             6:0.8,\n",
    "#                             7:0.8}\n",
    "#         self.oracle_for_target_fidelity={0:{0.7:2,0.8:3,0.9:2},\n",
    "#                              1:{0.7:2,0.8:3,0.9:2},\n",
    "#                             2:{0.7:3,0.8:4,0.9:2},\n",
    "#                              3:{0.7:3,0.8:4,0.9:2},\n",
    "#                             4:{0.7:3,0.8:4,0.9:2},\n",
    "#                             5:{0.7:3,0.8:4,0.9:2},\n",
    "#                             6:{0.7:3,0.8:4,0.9:2},\n",
    "#                             7:{0.7:2,0.8:4,0.9:2}} \n",
    "#         self.each_request_real_paths = {(1,6):[0],(2,5):[1,2],(10,12):[3]}\n",
    "#         self.each_request_virtual_paths = {(1,6):[3,4,7],(2,5):[6],(10,12):[]}\n",
    "#         self.storage_pairs = [(2,5),(10,12)]\n",
    "#         self.storage_nodes = [2,5,10,12]\n",
    "#         self.each_storage_capacity = {2:100,5:100,10:100,12:100}\n",
    "        \n",
    "#         #self.shortest_paths_file = self.topology_file +'_shortest_paths'\n",
    "#         #self.DG = nx.DiGraph()\n",
    "\n",
    "#         #self.load_topology()\n",
    "#         #self.calculate_paths()\n",
    "#     def get_required_purification_EPR_pairs(self,p,threshold):\n",
    "#         if threshold>=0.9:\n",
    "#             return self.oracle_for_target_fidelity[p][0.9]\n",
    "#         elif 0.8<threshold<0.9:\n",
    "#             return self.oracle_for_target_fidelity[p][0.8]\n",
    "#         elif threshold<0.8:\n",
    "#             return self.oracle_for_target_fidelity[p][0.7]\n",
    "#         else:\n",
    "#             return 1\n",
    "        \n",
    "#     def load_topology(self):\n",
    "#         print('[*] Loading topology...', self.topology_file)\n",
    "#         f = open(self.topology_file, 'r')\n",
    "#         header = f.readline()\n",
    "#         self.num_nodes = int(header[header.find(':')+2:header.find('\\t')])\n",
    "#         self.num_links = int(header[header.find(':', 10)+2:])\n",
    "#         f.readline()\n",
    "#         self.link_capacities = np.empty((self.num_links))\n",
    "#         self.link_weights = np.empty((self.num_links))\n",
    "#         for line in f:\n",
    "#             link = line.split('\\t')\n",
    "#             i, s, d, w, c = link\n",
    "#             print('this is our line',link,i, s, d, w, c)\n",
    "#             self.link_capacities[int(i)] = float(c)\n",
    "#             self.link_weights[int(i)] = int(w)\n",
    "#             self.DG.add_weighted_edges_from([(int(s),int(d),int(w))])\n",
    "#             self.set_E.append()\n",
    "        \n",
    "#         f.close()\n",
    "#         #print('nodes: %d, links: %d\\n'%(self.num_nodes, self.num_links))\n",
    "#         nx.draw_networkx(self.DG)\n",
    "#         plt.show()\n",
    "#     def check_path_include_sub_path(self,sub_path,path):\n",
    "#         if self.set_of_paths[sub_path] in self.set_of_paths[path]:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "#     def get_edges(self):\n",
    "#         return self.set_E\n",
    "#     def get_storage_capacity(self,storage):\n",
    "#         return self.each_storage_capacity[storage]\n",
    "#     def check_path_include_edge(self,edge,path):\n",
    "#         #print('edge is %s and path is %s and Paths is %s'%(edge,path,self.set_of_paths))\n",
    "#         if edge in self.set_of_paths[path]:\n",
    "#             return True\n",
    "#         elif edge not  in self.set_of_paths[path]:\n",
    "#             return False\n",
    "#     def check_storage_pair_exist(self,s1,s2):\n",
    "#         if (s1,s2) in self.storage_pairs:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "#     def check_request_use_path(self,k,p):\n",
    "#         if p in self.each_request_virtual_paths[k] or (p in self.each_request_virtual_paths[k]):\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "#     #         edge_capacity\n",
    "#     #         paths\n",
    "#     #         virtual_paths\n",
    "#     def get_path_length(self,path):\n",
    "#         return len(self.set_of_paths[path])\n",
    "#     def scale_network(self,each_edge_scaling):\n",
    "        \n",
    "#         for edge in self.set_E:\n",
    "#             self.each_edge_capacity[edge] = self.each_edge_capacity[edge]*each_edge_scaling\n",
    "            \n",
    "        \n",
    "# class Work_load(object):\n",
    "#     def __init__(self):\n",
    "#         self.T = [0,1,2]\n",
    "#         self.num_requests = 2\n",
    "#         self.request_pairs = [(1,6),(2,5),(10,12)]\n",
    "#         self.each_t_requests = {0:[(1,6),(2,5),(10,12)],1:[(1,6),(2,5),(10,12)],2:[(1,6),(2,5),(10,12)]}\n",
    "#         self.each_t_real_requests = {1:[(1,6)],2:[(1,6)]}\n",
    "#         self.time_intervals = 2\n",
    "#         self.each_t_each_request_demand = {0:{(1,6):10,(2,5):0,(10,12):0},\n",
    "#                                            1:{(1,6):1,(2,5):0,(10,12):0},2:{(1,6):10,(2,5):0,(10,12):0}}\n",
    "#         self.each_request_thrshold = {(1,6):{0:0.9,1:0.8,2:0.9},\n",
    "#                                      (2,5):{0:0.3,1:0.3,2:0.3},\n",
    "#                                      (10,12):{0:0.3,1:0.3,2:0.3}\n",
    "#                                      }\n",
    "#     def get_each_request_thrshold(self,k,t):\n",
    "#         return self.each_request_thrshold[k][t]\n",
    "#     def generate_workload_circle(self,alpha,T,request_pairs):\n",
    "#         new_t_request_demands = {}\n",
    "#         for t,request_demand in self.each_t_each_request_demand.items():\n",
    "#             new_t_request_demands[t] = {}\n",
    "#             for req,d in request_demand.items():\n",
    "#                 new_t_request_demands[t][req] = d*alpha\n",
    "#         for k,v in new_t_request_demands.items():\n",
    "#             self.each_t_each_request_demand[k] = v\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_csv_file(variable,file_name):\n",
    "    opt_df = pd.DataFrame.from_dict(variable, orient=\"index\", \n",
    "                                columns = [\"variable_object\"])\n",
    "    opt_df.index =  pd.MultiIndex.from_tuples(opt_df.index, \n",
    "                                   names=[\"column_i\", \"column_j\",\"column_k\"])\n",
    "    opt_df.reset_index(inplace=True)\n",
    "\n",
    "    opt_df[\"solution_value\"] =  opt_df[\"variable_object\"].apply(lambda item: item.solution_value)\n",
    "\n",
    "    opt_df.drop(columns=[\"variable_object\"], inplace=True)\n",
    "    opt_df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # connect a listener to the model\n",
    "# opt_model.add_progress_listener(TextProgressListener())\n",
    "# opt_model.solve(clean_before_solve=True);\n",
    "\n",
    "# for l, listener in enumerate(opt_model.iter_progress_listeners(), start=1):\n",
    "#     print(\"listener #{0} has type '{1}', clock={2}\".format(l, listener.__class__.__name__, listener.clock))\n",
    "# opt_model.clear_progress_listeners()\n",
    "# opt_model.add_progress_listener(TextProgressListener(clock='objective', absdiff=1, reldiff=0))\n",
    "# sol_recorder = SolutionRecorder()\n",
    "# opt_model.clear_progress_listeners()\n",
    "# opt_model.add_progress_listener(sol_recorder)\n",
    "# opt_model.solve(clean_before_solve=True);\n",
    "\n",
    "# # utility function to display recorded solutions in a recorder.\n",
    "# def display_recorded_solutions(rec):\n",
    "#     print('* The recorder contains {} solutions'.format(rec.number_of_solutions))\n",
    "#     for s, sol in enumerate(rec.iter_solutions(), start=1):\n",
    "#         sumvals = sum(v for _, v in sol.iter_var_values())\n",
    "#         print('  - solution #{0}, obj={1}, non-zero-values={2}, total={3}'.format(\n",
    "#            s, sol.objective_value, sol.number_of_var_values, sumvals))\n",
    "\n",
    "# display_recorded_solutions(sol_recorder)\n",
    "\n",
    "\n",
    "# sol_recorder2 = SolutionRecorder(clock='objective')\n",
    "# opt_model.clear_progress_listeners()\n",
    "# opt_model.add_progress_listener(sol_recorder2)\n",
    "# opt_model.solve(clean_before_solve=True)\n",
    "# display_recorded_solutions(sol_recorder2)\n",
    "# #large_hearts.add_progress_listener(TextProgressListener(clock='gap'))\n",
    "# # maximum non-improving time is 4 seconds.\n",
    "# opt_model.add_progress_listener(AutomaticAborter(max_no_improve_time=4))\n",
    "# # again use clean_before_solve to ensure deterministic run of this cell.\n",
    "# opt_model.solve(clean_before_solve=True, log_output=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"defining our decicion variables (that are integer)\n",
    "# store decision variables in Python dictionaries (or Pandas Series)\n",
    "# where dictionary keys are decision variables, and values are decision variable objects.\n",
    "# A decision variable is defined with three main properties: \n",
    "# its type (continuous, binary or integer), \n",
    "# its lower bound (0 by default), and \n",
    "# its upper bound (infinity by default)\"\"\"\n",
    "\n",
    "# x_vars  = {(i,j): opt_model.integer_var(lb=l[i,j], ub= u[i,j],\n",
    "#                               name=\"x_{0}_{1}\".format(i,j))  for i in set_I for j in set_J}\n",
    "\n",
    "# # print(x_vars)#{(1, 1): docplex.mp.Var(type=I,name='x_1_1',lb(lower bound)=2,ub(upper bound)=10), \n",
    "# ##(1, 2): docplex.mp.Var(type=I,name='x_1_2',ub=10) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"set constraints. \n",
    "# Any constraint has three parts: a left-hand side (normally a linear combination of decision variables),\n",
    "# a right-hand side (usually a numeric value), and\n",
    "# a sense (Less than or equal, Equal, or Greater than or equal).\n",
    "# To set up any constraints, we need to set each part:\"\"\"\n",
    "\n",
    "# # <= constraints\n",
    "# constraints = {j : \n",
    "# opt_model.add_constraint(\n",
    "#     ct=opt_model.sum(a[i,j] * x_vars[i,j] for i in set_I) <= b[j], ctname=\"constraint_{0}\".format(j)) for j in set_J}\n",
    "# # >= constraints\n",
    "# constraints = {j : opt_model.add_constraint(ct=opt_model.sum(a[i,j] * x_vars[i,j] for i in set_I) >= b[j], ctname=\"constraint_{0}\".format(j)) for j in set_J}\n",
    "# # == constraints\n",
    "# constraints = {j : \n",
    "# opt_model.add_constraint( ct=opt_model.sum(a[i,j] * x_vars[i,j] for i in set_I) == b[j], ctname=\"constraint_{0}\".format(j)) for j in set_J}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"we call the solver to solve our optimization model.\"\"\"\n",
    "# # solving with local cplex\n",
    "# opt_model.solve()\n",
    "\n",
    "# # solving with cplex cloud\n",
    "# # opt_model.solve(url=\"your_cplex_cloud_url\", key=\"your_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"get results and post-process them\n",
    "# If the problem is solved to optimality, we can get and process results as follows:\n",
    "# \"\"\"\n",
    "# import pandas as pd\n",
    "# opt_df = pd.DataFrame.from_dict(x_vars, orient=\"index\", \n",
    "#                                 columns = [\"variable_object\"])\n",
    "# opt_df.index =  pd.MultiIndex.from_tuples(opt_df.index, \n",
    "#                                names=[\"column_i\", \"column_j\"])\n",
    "# opt_df.reset_index(inplace=True)\n",
    "\n",
    "# opt_df[\"solution_value\"] =  opt_df[\"variable_object\"].apply(lambda item: item.solution_value)\n",
    "    \n",
    "# opt_df.drop(columns=[\"variable_object\"], inplace=True)\n",
    "# opt_df.to_csv(\"./optimization_solution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"\"\"input parameters\"\"\"\n",
    "# # import random\n",
    "# # n = 10\n",
    "# # m = 5\n",
    "# # set_I = range(1, n+1)\n",
    "# # set_J = range(1, m+1)\n",
    "# # # print(set_I)#range(1, 11)\n",
    "# # # print(set_J)# range(1, 6)\n",
    "\n",
    "# # c = {(i,j): random.normalvariate(0,1) for i in set_I for j in set_J}\n",
    "# # a = {(i,j): random.normalvariate(0,5) for i in set_I for j in set_J}\n",
    "# # l = {(i,j): random.randint(0,10) for i in set_I for j in set_J}\n",
    "# # u = {(i,j): random.randint(10,20) for i in set_I for j in set_J}\n",
    "# # b = {j: random.randint(0,30) for j in set_J}\n",
    "\n",
    "# # print(c) {(1, 1): #-0.03927470599644141, (1, 2): 1.0333198122747003, (1, 3): ....\n",
    "\n",
    "\n",
    "# from docplex.mp.progress import ProgressListener\n",
    "\n",
    "# class AutomaticAborter(ProgressListener):\n",
    "#     \"\"\" a simple implementation of an automatic search stopper.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, max_no_improve_time=10.):\n",
    "#         super(AutomaticAborter, self).__init__(ProgressClock.All)\n",
    "#         self.last_obj = None\n",
    "#         self.last_obj_time = None\n",
    "#         self.max_no_improve_time = max_no_improve_time\n",
    "        \n",
    "#     def notify_start(self):\n",
    "#         super(AutomaticAborter, self).notify_start()\n",
    "#         self.last_obj = None\n",
    "#         self.last_obj_time = None    \n",
    "        \n",
    "#     def is_improving(self, new_obj, eps=1e-4):\n",
    "#         last_obj = self.last_obj\n",
    "#         return last_obj is None or (abs(new_obj- last_obj) >= eps)\n",
    "            \n",
    "#     def notify_progress(self, pdata):\n",
    "#         super(AutomaticAborter, self).notify_progress(pdata)\n",
    "#         if pdata.has_incumbent and self.is_improving(pdata.current_objective):\n",
    "#             self.last_obj = pdata.current_objective\n",
    "#             self.last_obj_time = pdata.time\n",
    "#             print('----> #new objective={0}, time={1}s'.format(self.last_obj, self.last_obj_time))\n",
    "#         else:\n",
    "#             # a non improving move\n",
    "#             last_obj_time = self.last_obj_time\n",
    "#             this_time = pdata.time\n",
    "#             if last_obj_time is not None:\n",
    "#                 elapsed = (this_time - last_obj_time)\n",
    "#                 if elapsed >= self.max_no_improve_time:\n",
    "#                     print('!! aborting cplex, elapsed={0} >= max_no_improve: {1}'.format(elapsed,\n",
    "#                                                                              self.max_no_improve_time))\n",
    "#                     self.abort()\n",
    "#                 else:\n",
    "#                     print('----> non improving time={0}s'.format(elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
