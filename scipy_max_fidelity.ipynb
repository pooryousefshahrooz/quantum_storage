{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as spo\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from network import Network\n",
    "from workload import Work_load\n",
    "# from docplex.mp.progress import *\n",
    "# from docplex.mp.progress import SolutionRecorder\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from docplex.mp.progress import *\n",
    "from docplex.mp.progress import SolutionRecorder\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPLEX_feasibility(network,work_load,life_time,iteration,cyclic_workload,storage_capacity):\n",
    "    if cyclic_workload ==\"cyclic\":\n",
    "        cyclic_workload=True\n",
    "    else:\n",
    "        cyclic_workload= False\n",
    "    import docplex.mp.model as cpx\n",
    "    opt_model = cpx.Model(name=\"Storage problem model\"+str(iteration))\n",
    "    w_vars = {}\n",
    "    u_vars = {}\n",
    "    \n",
    "    w_vars  = {(t,k,p): opt_model.continuous_var(lb=0, ub= network.max_edge_capacity,\n",
    "                              name=\"w_{0}_{1}_{2}\".format(t,k,p))  for t in work_load.T \n",
    "               for k in work_load.each_t_requests[t] \n",
    "               for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]}\n",
    "\n",
    "    u_vars  = {(t,j,p): opt_model.continuous_var(lb=0, ub= network.max_edge_capacity,\n",
    "                                  name=\"u_{0}_{1}_{2}\".format(t,j,p))  for t in work_load.T \n",
    "                   for j in network.storage_pairs for p in network.each_request_real_paths[j]}   \n",
    "\n",
    "    if life_time ==1000:\n",
    "        #inventory evolution constraint\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    \n",
    " \n",
    "                    if cyclic_workload:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == u_vars[(t-1)%len(work_load.T),j,p_s]-\n",
    "                        opt_model.sum(w_vars[(t-1)%len(work_load.T),k,p]\n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                        +opt_model.sum(w_vars[(t-1)%len(work_load.T),j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "                    else:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == u_vars[t-1,j,p_s]-\n",
    "                        opt_model.sum(w_vars[t-1,k,p] \n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                        +opt_model.sum(w_vars[t-1,j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "    else:\n",
    "        #inventory evolution constraint\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    \n",
    "                    if cyclic_workload:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == -\n",
    "                        opt_model.sum(w_vars[(t-1)%len(work_load.T),k,p] \n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] \n",
    "                        )\n",
    "                        + opt_model.sum(w_vars[(t-1)%len(work_load.T),j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "                    else:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] == -\n",
    "                        opt_model.sum(w_vars[t-1,k,p] \n",
    "                        for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] \n",
    "                        )\n",
    "                        + opt_model.sum(w_vars[t-1,j,p_s])\n",
    "                                             , ctname=\"inventory_evolution_{0}_{1}\".format(t,j,p_s))\n",
    "\n",
    "    # serving from inventory constraint\n",
    "    for t in work_load.T[1:]:\n",
    "        for j in network.storage_pairs:\n",
    "            \n",
    "            for p_s in network.each_request_real_paths[j]:\n",
    "\n",
    "                opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                for k in work_load.each_t_requests[t] if k!=j for p in network.each_request_virtual_paths_include_subpath[k][p_s] if k in list(network.each_request_virtual_paths_include_subpath.keys()))<=u_vars[t,j,p_s]\n",
    "                                     , ctname=\"inventory_serving_{0}_{1}_{2}\".format(t,j,p_s))  \n",
    " \n",
    "     \n",
    "    # Demand constriant\n",
    "    for t in work_load.T[1:]:\n",
    "        for k in  work_load.each_t_requests[t]:\n",
    "            opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]) >= \n",
    "                    work_load.each_t_each_request_demand[t][k], ctname=\"constraint_{0}_{1}\".format(t,k))\n",
    "    \n",
    "    #Edge constraint\n",
    "    for t in work_load.T:\n",
    "        for edge in network.set_E:\n",
    "            opt_model.add_constraint(\n",
    "                opt_model.sum(w_vars[t,k,p] for k in work_load.each_t_requests[t]\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k] if network.check_path_include_edge(edge,p))\n",
    "                                     \n",
    "                 <= network.each_edge_capacity[edge], ctname=\"edge_capacity_{0}_{1}\".format(t,edge))\n",
    "     \n",
    "    # storage servers capacity constraint\n",
    "    for t in work_load.T:\n",
    "        for s1 in network.storage_nodes:\n",
    "            opt_model.add_constraint(opt_model.sum(u_vars[t,(s1,s2),p] \n",
    "                for s2 in network.storage_nodes if network.check_storage_pair_exist(s1,s2)\n",
    "                for p in network.each_request_real_paths[(s1,s2)])\n",
    "        <= storage_capacity, ctname=\"storage_capacity_constraint_{0}_{1}\".format(t,s1))\n",
    "    \n",
    "    # constraints for serving from storage at time zero and 1 should be zero\n",
    "    if not cyclic_workload:\n",
    "        for t in [0,1]:\n",
    "            opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                    for k in work_load.each_t_requests[t] for p in network.each_request_virtual_paths[k] \n",
    "                    )<=0, ctname=\"serving_from_inventory_{0}\".format(t))\n",
    "    \n",
    "    # constraints for putting in storage at time zero  should be zero\n",
    "    \"\"\"this is becasue we start the formulation from 1 and not from zero and we have t-1 in our formulation\"\"\"\n",
    "    for t in [0]:\n",
    "        opt_model.add_constraint(opt_model.sum(w_vars[t,k,p]\n",
    "                for k in work_load.each_t_requests[t] for p in network.each_request_real_paths[k] \n",
    "                )<=0, ctname=\"storing_in_inventory_{0}\".format(t))   \n",
    "    \n",
    "\n",
    "    # constraint for inventory is zero at time zero \n",
    "    if not cyclic_workload:\n",
    "        for t in [0]:\n",
    "            for j in network.storage_pairs:\n",
    "                 for p_s in network.each_request_real_paths[j]:\n",
    "                        opt_model.add_constraint(u_vars[t,j,p_s] <=0, ctname=\"storage_capacity_constraint_{0}_{1}_{2}\".format(t,j,p_s))\n",
    "    \n",
    "    \"\"\"defining an objective, which is a linear expression\"\"\"\n",
    "\n",
    "    objective = opt_model.sum(1/len(work_load.T[1:])*1/len(work_load.each_t_real_requests[t])*1/work_load.each_t_each_request_demand[t][k]\n",
    "                              *(w_vars[t,k,p] * network.get_path_length(p)) for t in work_load.T[1:]\n",
    "                              for k in work_load.each_t_real_requests[t] \n",
    "                              for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]\n",
    "                              )\n",
    "\n",
    "    \n",
    "  \n",
    "    opt_model.minimize(objective)\n",
    "    \n",
    "    opt_model.print_information()\n",
    "    \n",
    "    opt_model.solve()\n",
    "\n",
    "    \n",
    "    #print('docplex.mp.solution',opt_model.solution)\n",
    "    objective_value = -1\n",
    "    try:\n",
    "        if opt_model.solution:\n",
    "            objective_value =opt_model.solution.get_objective_value()\n",
    "    except ValueError:\n",
    "        print(ValueError)\n",
    "    each_time_each_path_delivered_EPRs = {}\n",
    "    if objective_value>0:\n",
    "        for t in work_load.T[1:]:\n",
    "            for k in work_load.each_t_real_requests[t]: \n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                    try:\n",
    "                        each_time_each_path_delivered_EPRs[t][k][p]=w_vars[t,k,p].solution_value\n",
    "                    except:\n",
    "                        try:\n",
    "                            each_time_each_path_delivered_EPRs[t][k]= {}\n",
    "                            each_time_each_path_delivered_EPRs[t][k][p]=w_vars[t,k,p].solution_value\n",
    "                        except:\n",
    "                            each_time_each_path_delivered_EPRs[t]={}\n",
    "                            each_time_each_path_delivered_EPRs[t][k]= {}\n",
    "                            each_time_each_path_delivered_EPRs[t][k][p]=w_vars[t,k,p].solution_value\n",
    "    \n",
    "        \n",
    "       \n",
    "    opt_model.clear()\n",
    "   \n",
    "    return objective_value,each_time_each_path_delivered_EPRs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizing_avg_fidelity(network,work_load,life_time,iteration,storage_capacity,max_allowed_purifiying_EPRs):\n",
    "    wn_start = []\n",
    "    bnds = []\n",
    "    # Constraints\n",
    "    # edge constraint\n",
    "    # print(\"each_t_k_p_w_vars_indx\",each_t_k_p_w_vars_indx)\n",
    "    # print(\"each_t_k_p_n_vars_indx\",each_t_k_p_n_vars_indx)\n",
    "\n",
    "    for t in work_load.T:\n",
    "        for k in work_load.each_t_requests[t]:\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                wn_start.append(20)\n",
    "                wn_start.append(10)\n",
    "                bnds.append((0,network.max_edge_capacity))\n",
    "                bnds.append((1,max_allowed_purifiying_EPRs))\n",
    "        for j in network.storage_pairs:\n",
    "            for p_s in network.each_request_real_paths[j]:\n",
    "                wn_start.append(network.max_edge_capacity)\n",
    "                bnds.append((0,storage_capacity))\n",
    "\n",
    "    indx = 0\n",
    "    each_t_k_p_n_vars_indx = {}\n",
    "    each_t_k_p_w_vars_indx = {}\n",
    "    each_t_k_p_u_vars_indx = {}\n",
    "    for t in work_load.T:\n",
    "        for k in work_load.each_t_requests[t]:\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                each_t_k_p_w_vars_indx[t,k,p] = indx\n",
    "                indx+=1\n",
    "                each_t_k_p_n_vars_indx[t,k,p] = indx\n",
    "                indx+=1\n",
    "        for j in network.storage_pairs:\n",
    "            for p_s in network.each_request_real_paths[j]:\n",
    "                each_t_k_p_u_vars_indx[t,j,p_s] = indx\n",
    "                indx+=1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Constraints\n",
    "    # edge constraint\n",
    "    # print(\"each_t_k_p_w_vars_indx\",each_t_k_p_w_vars_indx)\n",
    "    # print(\"each_t_k_p_n_vars_indx\",each_t_k_p_n_vars_indx)\n",
    "\n",
    "    def edge_constraint(wn,w_indxes,n_indxes,edge,edge_capacity):\n",
    "        index_tracker = 0\n",
    "        edge_load = 0\n",
    "        #print(\"for edge %s capacity %s we need to check %s %s\"%(edge,edge_capacity,w_indxes,n_indxes ))\n",
    "        for indx in w_indxes:\n",
    "            edge_load= edge_load+wn[indx]*wn[n_indxes[index_tracker]]\n",
    "            index_tracker+=1\n",
    "    #     if edge_capacity-edge_load>0:\n",
    "    #         print(\"for edge %s capacity %s we have load %s constraint %s\"%(edge,edge_capacity,edge_load,edge_capacity-edge_load))\n",
    "        return edge_capacity-edge_load\n",
    "\n",
    "    def demand_constraint(wn,w_indxes,t,k,demand):\n",
    "        served_EPRs = 0\n",
    "        #print(\"for time %s request %s demand %s\"%(t,k,demand))\n",
    "        for indx in w_indxes:\n",
    "            served_EPRs= served_EPRs+wn[indx]\n",
    "    #     print(\"served_EPRs %s and demand is %s\"%(served_EPRs,demand))\n",
    "        return served_EPRs-demand\n",
    "    def inventory_constraint(wn,u_t_j_p_s_indx,u_t_1_j_p_s_indx,w_vars_t_1_j_p_s_indx,w_indxes,n_indxes):\n",
    "        served_from_storage = 0\n",
    "        index_tracker = 0\n",
    "        #print('1 w',len(w_indxes),w_indxes)\n",
    "        #print('1 n',len(n_indxes),n_indxes)\n",
    "        for indx in w_indxes:\n",
    "            served_from_storage= served_from_storage+wn[indx]*wn[n_indxes[index_tracker]]\n",
    "            index_tracker+=1\n",
    "        stored_at_storage = wn[w_vars_t_1_j_p_s_indx]\n",
    "        inventory_result =  wn[u_t_1_j_p_s_indx]-served_from_storage+stored_at_storage-wn[u_t_j_p_s_indx] \n",
    "        return inventory_result\n",
    "    def inventory_constraint_one_time_slot(wn,u_t_j_p_s_indx,w_vars_t_1_j_p_s_indx,w_indxes,n_indxes):\n",
    "        served_from_storage = 0\n",
    "        index_tracker = 0\n",
    "\n",
    "        for indx in w_indxes:\n",
    "            served_from_storage= served_from_storage+wn[indx]*wn[n_indxes[index_tracker]]\n",
    "            index_tracker+=1\n",
    "        stored_at_storage = wn[w_vars_t_1_j_p_s_indx]\n",
    "        inventory_result =  -served_from_storage+stored_at_storage-wn[u_t_j_p_s_indx] \n",
    "        return inventory_result\n",
    "    def serving_from_inventory_constraint(wn,u_t_j_p_s_indx,w_indxes,n_indxes):\n",
    "        served_from_storage = 0\n",
    "        index_tracker = 0\n",
    "        #print('2 w',len(w_indxes),w_indxes)\n",
    "        #print('2 n',len(n_indxes),n_indxes)\n",
    "        for indx in w_indxes:\n",
    "            served_from_storage= served_from_storage+wn[indx]*wn[n_indxes[index_tracker]]\n",
    "            index_tracker+=1\n",
    "        inventory_usage =  wn[u_t_j_p_s_indx]-served_from_storage\n",
    "        return inventory_usage\n",
    "\n",
    "    def storage_capacity_constraint(wn,w_indxes,storage_capacity):\n",
    "        served_from_storage = 0\n",
    "        for indx in w_indxes:\n",
    "            served_from_storage= served_from_storage+wn[indx]\n",
    "        return storage_capacity-served_from_storage\n",
    "    def serving_from_storage_time_zero(wn,w_indxes):\n",
    "        served_from_storage = 0\n",
    "        for indx in w_indxes:\n",
    "            served_from_storage= served_from_storage+wn[indx]\n",
    "        return served_from_storage-0\n",
    "    def storage_at_time_zero_constraint(wn,u_index):\n",
    "        storage_at_time_zero = wn[u_index]\n",
    "        return storage_at_time_zero-0\n",
    "    def placing_at_storage_at_time_zero_constraint(wn,w_indxes):\n",
    "        placed_at_storage = 0\n",
    "        for indx in w_indxes:\n",
    "            placed_at_storage= placed_at_storage+wn[indx]\n",
    "        return placed_at_storage-0\n",
    "    constraints = []\n",
    "    # inventory constraints\n",
    "    if life_time ==1000:\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    w_indxes = []\n",
    "                    n_indxes = []\n",
    "                    for k in work_load.each_t_requests[t]:\n",
    "                        if k!=j:\n",
    "                            for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "                                w_indxes.append(each_t_k_p_w_vars_indx[t-1,k,p])\n",
    "                                n_indxes.append(each_t_k_p_n_vars_indx[t-1,k,p])\n",
    "                    u_t_j_p_s_indx = each_t_k_p_u_vars_indx[t,j,p_s]\n",
    "                    u_t_1_j_p_s_indx = each_t_k_p_u_vars_indx[t-1,j,p_s]\n",
    "                    w_vars_t_1_j_p_s_indx = each_t_k_p_w_vars_indx[t-1,j,p_s]\n",
    "                    constraints.append({'type': 'eq', 'fun':inventory_constraint,'args':(u_t_j_p_s_indx,u_t_1_j_p_s_indx,w_vars_t_1_j_p_s_indx,w_indxes,n_indxes,)})\n",
    "    else:\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p_s in network.each_request_real_paths[j]:\n",
    "                    w_indxes = []\n",
    "                    n_indxes = []\n",
    "                    for k in work_load.each_t_requests[t]:\n",
    "                        if k!=j:\n",
    "                            for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "                                w_indxes.append(each_t_k_p_w_vars_indx[t-1,k,p])\n",
    "                                n_indxes.append(each_t_k_p_n_vars_indx[t-1,k,p])\n",
    "                    u_t_j_p_s_indx = each_t_k_p_u_vars_indx[t,j,p_s]\n",
    "                    u_t_1_j_p_s_indx = each_t_k_p_u_vars_indx[t-1,j,p_s]\n",
    "                    w_vars_t_1_j_p_s_indx = each_t_k_p_w_vars_indx[t-1,j,p_s]\n",
    "                    constraints.append({'type': 'eq', 'fun':inventory_constraint_one_time_slot,'args':(u_t_j_p_s_indx,w_vars_t_1_j_p_s_indx,w_indxes,n_indxes,)})\n",
    "    # serving from inventory constraint\n",
    "    for t in work_load.T[1:]:\n",
    "        for j in network.storage_pairs:\n",
    "            for p_s in network.each_request_real_paths[j]:\n",
    "                w_indxes=[]\n",
    "                n_indxes=[]\n",
    "                for k in work_load.each_t_requests[t]:\n",
    "                    if k!=j:\n",
    "                        for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "                            if k in list(network.each_request_virtual_paths_include_subpath.keys()):\n",
    "                                w_indxes.append(each_t_k_p_w_vars_indx[t,k,p])\n",
    "                                n_indxes.append(each_t_k_p_n_vars_indx[t,k,p])\n",
    "                #print(\"3 w\",len(w_indxes),w_indxes)\n",
    "                #print(\"3 n\",len(n_indxes),n_indxes)\n",
    "                constraints.append({'type': 'eq', 'fun':serving_from_inventory_constraint,'args':(u_t_j_p_s_indx,w_indxes,n_indxes,)})\n",
    "    # edge constraint\n",
    "    for t in work_load.T:\n",
    "        for edge in network.set_E:\n",
    "            w_indxes = []\n",
    "            n_indxes = [] \n",
    "            for k in work_load.each_t_requests[t]:\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                    if network.check_path_include_edge(edge,p):\n",
    "                        w_indxes.append(each_t_k_p_w_vars_indx[t,k,p])\n",
    "                        n_indxes.append(each_t_k_p_n_vars_indx[t,k,p])\n",
    "            constraints.append({'type': 'ineq', 'fun': edge_constraint, 'args': (w_indxes,n_indxes,edge,network.each_edge_capacity[edge],)})\n",
    "    # Demand constriant\n",
    "    for t in work_load.T[1:]:\n",
    "        for k in  work_load.each_t_requests[t]:\n",
    "            w_indxes = []\n",
    "            for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                w_indxes.append(each_t_k_p_w_vars_indx[t,k,p])\n",
    "            constraints.append({'type': 'ineq', 'fun':demand_constraint,'args':(w_indxes,t,k,work_load.each_t_each_request_demand[t][k],)})\n",
    "\n",
    "    # storage servers capacity constraint\n",
    "    for t in work_load.T:\n",
    "        for s1 in network.storage_nodes:\n",
    "            w_indxes = []\n",
    "            for s2 in network.storage_nodes:\n",
    "                if network.check_storage_pair_exist(s1,s2):\n",
    "                    for p in network.each_request_real_paths[(s1,s2)]:\n",
    "                        w_indxes.append(each_t_k_p_u_vars_indx[t,(s1,s2),p])\n",
    "            constraints.append({'type': 'ineq', 'fun':storage_capacity_constraint,'args':(w_indxes,storage_capacity,)})\n",
    "\n",
    "\n",
    "    # constraints for serving from storage at time zero and 1? should be zero\n",
    "    for t in [0]:\n",
    "        w_indxes = []\n",
    "        for k in work_load.each_t_requests[t]:\n",
    "            for p in network.each_request_virtual_paths[k]:\n",
    "                w_indxes.append(each_t_k_p_w_vars_indx[t,k,p])\n",
    "        constraints.append({'type': 'eq', 'fun':serving_from_storage_time_zero,'args':(w_indxes,)})\n",
    "\n",
    "\n",
    "    # constraints for putting in storage at time zero should be zero\n",
    "    \"\"\"this is becasue we start the formulation from 1 and not from zero and we have t-1 in our formulation\"\"\"\n",
    "    for t in [0]:\n",
    "        w_indxes = []\n",
    "        for k in work_load.each_t_requests[t]:\n",
    "            for p in network.each_request_real_paths[k]:\n",
    "                w_indxes.append(each_t_k_p_w_vars_indx[t,k,p])\n",
    "        constraints.append({'type': 'eq', 'fun':placing_at_storage_at_time_zero_constraint,'args':(w_indxes,)})\n",
    "\n",
    "\n",
    "\n",
    "    # constraint for inventory is zero at time zero \n",
    "    for t in [0]:\n",
    "        for j in network.storage_pairs:\n",
    "             for p_s in network.each_request_real_paths[j]:\n",
    "                    u_index=each_t_k_p_u_vars_indx[t,j,p_s]\n",
    "                    constraints.append({'type': 'eq', 'fun':storage_at_time_zero_constraint,'args':(u_index,)})\n",
    "\n",
    "    def get_path_purified_fidelity(n,F_p):\n",
    "        f = round(F_p,4)\n",
    "        n = int(n)\n",
    "        #print(\"purifiying %s with %s EPR pairs\"%(f,n))\n",
    "        if (int(n),f) in network.each_n_f_purification_result:\n",
    "            #print(\"we used from memeory!\")\n",
    "            return network.each_n_f_purification_result[(int(n),f)]\n",
    "        else:\n",
    "            fidelity = network.recursive_purification(n,f)\n",
    "            network.each_n_f_purification_result[(int(n),f)] =  round(fidelity,4)\n",
    "            return fidelity\n",
    "    def objective_function_test(wn):\n",
    "#         for t in work_load.T[1:]:\n",
    "#             for k in work_load.each_t_real_requests[t]:\n",
    "#                 for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "#                     print(\"for t %s k %s p %s we have w=%s n=%s demand %s\"%(t,k,p,wn[each_t_k_p_w_vars_indx[t,k,p]],wn[each_t_k_p_n_vars_indx[t,k,p]],work_load.each_t_each_request_demand[t][k]))\n",
    "#         for t in work_load.T[1:]:\n",
    "#             for j in network.storage_pairs:\n",
    "#                 for p_s in network.each_request_real_paths[j]:\n",
    "#                     print(\"for t %s j %s p_s %s we have u=%s \"%(t,j,p_s,wn[each_t_k_p_u_vars_indx[t,j,p_s]]))\n",
    "\n",
    "    #     print(\"bnds\",bnds)\n",
    "    #     for t in T[1:]:\n",
    "    #         for k in each_t_real_requests[t]:\n",
    "    #             for p in each_request_real_paths[k]+each_request_virtual_paths[k]:\n",
    "    #                 print(\"for t %s k %s p %s we have start w=%s n=%s     \"%(t,k,p,  wn[each_t_k_p_w_vars_indx[t,k,p]],  wn[each_t_k_p_n_vars_indx[t,k,p]]))\n",
    "    #                 print(\"for t %s k %s p %s we have bound w=%s n=%s\"%(t,k,p,bnds[each_t_k_p_w_vars_indx[t,k,p]],bnds[each_t_k_p_n_vars_indx[t,k,p]]))\n",
    "\n",
    "    #         for j in storage_pairs:\n",
    "    #             for p_s in each_request_real_paths[j]:\n",
    "    #                 print(\"for t %s j %s p_s %s we have bound u=%s \"%(t,j,p_s,bnds[each_t_k_p_u_vars_indx[t,j,p_s]]))\n",
    "\n",
    "    #     time.sleep(4)\n",
    "        w_vars = {}\n",
    "        n_vars= {}\n",
    "        indx = 0\n",
    "        for t in work_load.T:\n",
    "            for k in work_load.each_t_requests[t]:\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                    w_vars[t,k,p] = wn[each_t_k_p_w_vars_indx[t,k,p]]\n",
    "                    indx+=1\n",
    "                    n_vars[t,k,p] = wn[each_t_k_p_n_vars_indx[t,k,p]]\n",
    "                    indx+=1\n",
    "        #print(\"each_t_k_p_w_vars_indx\",each_t_k_p_w_vars_indx)\n",
    "        #print(\"w_vars\",w_vars)\n",
    "        each_t_fidelities = []\n",
    "        for t in work_load.T[1:]:\n",
    "            for k in work_load.each_t_real_requests[t]:\n",
    "                served_EPRs = 0\n",
    "                this_k_purified_fidelities = []\n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                    purified_fidelity = get_path_purified_fidelity(n_vars[t,k,p],network.each_path_basic_fidelity[p])\n",
    "                    #print(\"purified_fidelity \",purified_fidelity)\n",
    "                    weighted_purified_fidelity = w_vars[t,k,p]*purified_fidelity\n",
    "                    #print(\"weighted_purified_fidelity \",weighted_purified_fidelity)\n",
    "                    this_k_purified_fidelities.append(weighted_purified_fidelity)\n",
    "                    served_EPRs = served_EPRs+w_vars[t,k,p]\n",
    "            #each_t_fidelities.append(sum(this_k_purified_fidelities)/each_t_each_request_demand[t][k])\n",
    "            each_t_fidelities.append(sum(this_k_purified_fidelities)/served_EPRs)\n",
    "        final_avg_fidelity = sum(each_t_fidelities)/len(each_t_fidelities)\n",
    "    #     *(w_vars[t,k,p] * recursive_purification(n_vars[t,k,p],each_p_fidelity[p])) \n",
    "    #     for t in T[1:] for k in each_t_real_requests[t] for p in each_request_real_paths[k]+each_request_virtual_paths[k]\n",
    "        return -final_avg_fidelity\n",
    "    objective_value = -1\n",
    "    fail_flag = False\n",
    "    solver_flag = False\n",
    "    try:\n",
    "        result = spo.minimize(objective_function_test,wn_start,method=\"SLSQP\",options = {\"disp\":True,\"tol\":0.01},constraints = constraints, bounds = bnds)\n",
    "        #print result\n",
    "        if result.success:\n",
    "            print(\"success!\")\n",
    "            solver_flag = True\n",
    "            print(\"final fidelity is \",result.fun)\n",
    "            objective_value = -(result.fun)\n",
    "            wn = result.x\n",
    "            w0 = wn[0]\n",
    "            n0 = wn[1]\n",
    "            w1 = wn[2]\n",
    "            n1 = wn[3]\n",
    "        #     print(f\"w0={w0} n0={n0} w1={w1} n1={n1}\")\n",
    "            #print(\"wn is %s \"%wn)\n",
    "            for t in work_load.T[1:]:\n",
    "                for k in work_load.each_t_real_requests[t]:\n",
    "                    \n",
    "                    for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                        print(\"for t %s k %s p %s we have w=%s n=%s demand %s\"%(t,k,p,wn[each_t_k_p_w_vars_indx[t,k,p]],wn[each_t_k_p_n_vars_indx[t,k,p]],work_load.each_t_each_request_demand[t][k]))\n",
    "            for t in work_load.T[1:]:\n",
    "                for j in network.storage_pairs:\n",
    "                    for p_s in network.each_request_real_paths[j]:\n",
    "                        print(\"for t %s j %s p_s %s we have u=%s \"%(t,j,p_s,wn[each_t_k_p_u_vars_indx[t,j,p_s]]))\n",
    "        \n",
    "        else:\n",
    "            print(\"Sorry, could not find a minimum.\") \n",
    "            solver_flag = False\n",
    "            fail_flag = True\n",
    "            objective_value = -(result.fun)\n",
    "    except ValueError:\n",
    "        print(ValueError)\n",
    "#         objective_value = -1\n",
    "\n",
    "    each_inventory_per_time_usage = {}\n",
    "    each_time_each_path_delivered_EPRs = {}\n",
    "    each_time_each_path_purification_EPRs = {}\n",
    "    if objective_value>0 and not fail_flag:\n",
    "        for t in work_load.T[1:]:\n",
    "            for k in work_load.each_t_real_requests[t]: \n",
    "                for p in network.each_request_real_paths[k]+network.each_request_virtual_paths[k]:\n",
    "                    try:\n",
    "                        each_time_each_path_delivered_EPRs[t][k]+=wn[each_t_k_p_w_vars_indx[t,k,p]]\n",
    "                    except:\n",
    "                        try:\n",
    "                            each_time_each_path_delivered_EPRs[t][k]= wn[each_t_k_p_w_vars_indx[t,k,p]]\n",
    "                        except:\n",
    "                            each_time_each_path_delivered_EPRs[t]={}\n",
    "                            each_time_each_path_delivered_EPRs[t][k]= wn[each_t_k_p_w_vars_indx[t,k,p]]\n",
    "        \n",
    "                    try:\n",
    "                        each_time_each_path_purification_EPRs[t][k]+=wn[each_t_k_p_n_vars_indx[t,k,p]]\n",
    "                    except:\n",
    "                        try:\n",
    "                            each_time_each_path_purification_EPRs[t][k]= wn[each_t_k_p_n_vars_indx[t,k,p]]\n",
    "                        except:\n",
    "                            each_time_each_path_purification_EPRs[t]={}\n",
    "                            each_time_each_path_purification_EPRs[t][k]= wn[each_t_k_p_n_vars_indx[t,k,p]]\n",
    "        for t in work_load.T[1:]:\n",
    "            for j in network.storage_pairs:\n",
    "                for p in network.each_request_real_paths[j]:\n",
    "                    try:\n",
    "                        each_inventory_per_time_usage[j][t]=wn[each_t_k_p_u_vars_indx[t,j,p]]\n",
    "                    except:\n",
    "                        try:\n",
    "                            each_inventory_per_time_usage[j][t]=wn[each_t_k_p_u_vars_indx[t,j,p]]\n",
    "                        except:\n",
    "                            each_inventory_per_time_usage[j] = {}\n",
    "                            each_inventory_per_time_usage[j][t]=wn[each_t_k_p_u_vars_indx[t,j,p]]    \n",
    "        \n",
    "   \n",
    "   \n",
    "    return objective_value,each_inventory_per_time_usage,each_time_each_path_delivered_EPRs,each_time_each_path_purification_EPRs,solver_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizing_fidelity(each_network_topology_file,results_file_path,inventory_utilization_results_file_path,delived_purification_EPRs_file_path,number_of_user_pairs,number_of_time_slots, spike_means,num_spikes,experiment_repeat,storage_node_selection_schemes,fidelity_threshold_ranges,cyclic_workload,storage_capacities,given_life_time_set,distance_between_users,setting_demands,max_allowed_purifiying_EPRs):\n",
    "    print(\"each_network_topology_file %s \\n ,results_file_path %s , inventory_utilization_results_file_path %s ,number_of_user_pairs %s ,number_of_time_slots %s , spike_means %s ,num_spikes %s ,experiment_repeat %s ,storage_node_selection_schemes %s ,fidelity_threshold_ranges %s ,cyclic_workload %s ,storage_capacities %s ,given_life_time_set %s ,distance_between_users %s\"%(each_network_topology_file,results_file_path,inventory_utilization_results_file_path,number_of_user_pairs,number_of_time_slots, spike_means,num_spikes,experiment_repeat,storage_node_selection_schemes,fidelity_threshold_ranges,cyclic_workload,storage_capacities,given_life_time_set,distance_between_users))\n",
    "    \n",
    "    for network_topology,file_path in each_network_topology_file.items():\n",
    "        for spike_mean in spike_means:\n",
    "            import pdb\n",
    "            each_storage_each_path_number_value = {}\n",
    "            network = Network(file_path)\n",
    "            \n",
    "            for i in range(experiment_repeat):\n",
    "                network.reset_storage_pairs()\n",
    "                network.get_user_pairs(number_of_user_pairs,distance_between_users,number_of_time_slots)\n",
    "                work_load = Work_load(number_of_time_slots,\"time_demands_file.csv\")\n",
    "                \"\"\"we set the demands for each user pair\"\"\"\n",
    "                if setting_demands==\"python_library\":\n",
    "                    work_load.set_each_user_pair_demands(number_of_time_slots,network.each_t_user_pairs,spike_mean,num_spikes)\n",
    "                else:\n",
    "                    work_load.set_each_user_pair_demands_randomly(number_of_time_slots,network.each_t_user_pairs,spike_mean,num_spikes)\n",
    "                \"\"\"we set at least one demand for each time to avoid divided by zero error\"\"\"\n",
    "                work_load.check_demands_per_each_time(network.each_t_user_pairs)                                      \n",
    "                for storage_capacity in storage_capacities:\n",
    "                    for fidelity_threshold_range in fidelity_threshold_ranges:\n",
    "                        network.set_user_pair_fidelity_threshold(fidelity_threshold_range)\n",
    "                        for storage_node_selection_scheme in storage_node_selection_schemes:\n",
    "\n",
    "                            objective_values = []\n",
    "                            selected_storage_nodes = []\n",
    "                            selected_storage_pairs = []\n",
    "\n",
    "                            #nx.draw(network.g,with_labels=True)\n",
    "                            # plt.show()\n",
    "                            for num_paths in [3]:\n",
    "                                \n",
    "                                for life_time in given_life_time_set:\n",
    "                                    current_available_fidelities=[]\n",
    "                                    permited_work_load = False\n",
    "                                    for number_of_storages in [0,2,3,4,5,6]:\n",
    "                                        if number_of_storages ==0 or (number_of_storages>0 and permited_work_load):\n",
    "                                            network.each_request_real_paths = {}\n",
    "                                            network.reset_pair_paths()\n",
    "                                            \"\"\"with new storage pairs, we will check the solution for each number of paths(real and virtual)\"\"\"\n",
    "                                            work_load.reset_variables()\n",
    "                                            pairs = []\n",
    "                                            for t,user_pairs in network.each_t_user_pairs.items():            \n",
    "                                                for user_pair in user_pairs:\n",
    "                                                    if user_pair not in pairs:\n",
    "                                                        pairs.append(user_pair)\n",
    "                                            network.get_each_user_pair_real_paths(pairs)\n",
    "\n",
    "                                            path_counter_id = 0\n",
    "                                            \"\"\"select and add new storage pairs\"\"\"\n",
    "                                            network.get_new_storage_pairs(number_of_storages,storage_node_selection_scheme)\n",
    "\n",
    "                                            work_load.set_each_time_requests(network.each_t_user_pairs,network.storage_pairs)\n",
    "                                            work_load.set_each_time_real_requests(network.each_t_user_pairs)\n",
    "\n",
    "                                            network.get_each_user_pair_real_paths(network.storage_pairs)\n",
    "                                            if number_of_storages==1:\n",
    "                                                number_of_storages = 2\n",
    "\n",
    "                                            \"\"\"first we add the real paths between storage pairs\"\"\"\n",
    "                                            for storage_pair in network.storage_pairs:\n",
    "                                                paths = network.get_real_path(storage_pair,num_paths)\n",
    "                                                for path in paths:\n",
    "                                                    network.set_each_path_length(path_counter_id,path)\n",
    "                                                    network.set_of_paths[path_counter_id] = path\n",
    "                                                    network.each_path_path_id[tuple(path)] = path_counter_id\n",
    "                                                    try:\n",
    "                                                        network.each_request_real_paths[storage_pair].append(path_counter_id)\n",
    "                                                    except:\n",
    "                                                        network.each_request_real_paths[storage_pair]=[path_counter_id]\n",
    "                                                    try:\n",
    "                                                        network.each_storage_real_paths[storage_pair].append(path)\n",
    "                                                    except:\n",
    "                                                        network.each_storage_real_paths[storage_pair]=[path]\n",
    "                                                    path_counter_id+=1\n",
    "\n",
    "                                            across_all_time_slots_pairs = []\n",
    "                                            for t,user_pairs in network.each_t_user_pairs.items():\n",
    "                                                for user_pair in user_pairs:\n",
    "                                                    if user_pair not in across_all_time_slots_pairs:\n",
    "                                                        across_all_time_slots_pairs.append(user_pair)\n",
    "                                            all_sub_paths = []\n",
    "                                            for user_pair in across_all_time_slots_pairs:\n",
    "                                                paths = network.get_real_path(user_pair,num_paths)\n",
    "                                                for path in paths:\n",
    "                                                    network.set_of_paths[path_counter_id] = path\n",
    "                                                    network.set_each_path_length(path_counter_id,path)\n",
    "                                                    network.each_path_path_id[tuple(path)] = path_counter_id\n",
    "                                                    try:\n",
    "                                                        network.each_request_real_paths[user_pair].append(path_counter_id)\n",
    "                                                    except:\n",
    "                                                        network.each_request_real_paths[user_pair]=[path_counter_id]\n",
    "                                                    path_counter_id+=1\n",
    "\n",
    "                                                for storage_pair in network.storage_pairs:\n",
    "                                                    \"\"\"add one new path to the previous paths\"\"\"\n",
    "\n",
    "                                                    for real_sub_path in network.each_storage_real_paths[storage_pair]:\n",
    "\n",
    "                                                        paths = network.get_paths_to_connect_users_to_storage(user_pair,real_sub_path,num_paths)\n",
    "\n",
    "                                                        this_sub_path_id = network.each_path_path_id[tuple(real_sub_path)]\n",
    "\n",
    "                                                        for path in paths:\n",
    "                                                            path = network.remove_storage_pair_real_path_from_path(real_sub_path,path)\n",
    "                                                            network.set_each_path_length(path_counter_id,path)\n",
    "                                                            \"\"\"we remove the sub path that is connecting two storage pairs \n",
    "                                                            from the path because we do not want to check the edge capacity for the edges of this subpath\"\"\"\n",
    "                                                            try:\n",
    "                                                                network.each_request_virtual_paths_include_subpath[user_pair][this_sub_path_id].append(path_counter_id)\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    network.each_request_virtual_paths_include_subpath[user_pair][this_sub_path_id]=[path_counter_id]\n",
    "                                                                except:\n",
    "                                                                    network.each_request_virtual_paths_include_subpath[user_pair]={}\n",
    "                                                                    network.each_request_virtual_paths_include_subpath[user_pair][this_sub_path_id]=[path_counter_id]\n",
    "\n",
    "                                                            if this_sub_path_id not in all_sub_paths:\n",
    "                                                                all_sub_paths.append(this_sub_path_id)\n",
    "\n",
    "                                                            network.set_of_paths[path_counter_id] = path\n",
    "                                                            try:\n",
    "                                                                network.each_request_virtual_paths[user_pair].append(path_counter_id)\n",
    "                                                            except:\n",
    "                                                                network.each_request_virtual_paths[user_pair]=[path_counter_id]\n",
    "\n",
    "                                                            path_counter_id+=1\n",
    "\n",
    "\n",
    "                                                        for pair in network.storage_pairs:\n",
    "                                                            network.each_request_virtual_paths[pair]=[]\n",
    "\n",
    "                                            if number_of_storages==0:\n",
    "                                                for t,pairs in network.each_t_user_pairs.items():\n",
    "                                                    for pair in pairs:\n",
    "                                                        network.each_request_virtual_paths[pair]=[]\n",
    "                                                        for j in network.storage_pairs:\n",
    "                                                            for sub_path_id in all_sub_paths:\n",
    "                                                                network.each_request_virtual_paths_include_subpath[pair][sub_path_id]={}\n",
    "                                            for j in network.storage_pairs:\n",
    "                                                for sub_path_id in all_sub_paths:\n",
    "                                                    try:\n",
    "                                                        network.each_request_virtual_paths_include_subpath[j][sub_path_id] = []\n",
    "                                                    except:\n",
    "                                                        network.each_request_virtual_paths_include_subpath[j]={}\n",
    "                                                        network.each_request_virtual_paths_include_subpath[j][sub_path_id] = []\n",
    "                                            for t in range(number_of_time_slots):\n",
    "                                                for k in work_load.each_t_requests[t]:\n",
    "                                                    for sub_path_id in all_sub_paths:\n",
    "                                                        try:\n",
    "                                                            if k in list(network.each_request_virtual_paths_include_subpath.keys()):\n",
    "                                                                if sub_path_id not in list(network.each_request_virtual_paths_include_subpath[k].keys()):\n",
    "\n",
    "                                                                    try:\n",
    "                                                                        network.each_request_virtual_paths_include_subpath[k][sub_path_id] = []\n",
    "                                                                    except:\n",
    "                                                                        network.each_request_virtual_paths_include_subpath[k]={}\n",
    "                                                                        network.each_request_virtual_paths_include_subpath[k][sub_path_id] = []\n",
    "                                                                else:\n",
    "                                                                    action= \"do nothing!\"\n",
    "                                                            else:\n",
    "                                                                for sub_path_id in all_sub_paths:\n",
    "                                                                    try:\n",
    "                                                                        network.each_request_virtual_paths_include_subpath[k][sub_path_id] = []\n",
    "                                                                    except:\n",
    "                                                                        network.each_request_virtual_paths_include_subpath[k]={}\n",
    "                                                                        network.each_request_virtual_paths_include_subpath[k][sub_path_id] = []\n",
    "                                                        except:\n",
    "                                                            for sub_path_id in all_sub_paths:\n",
    "                                                                try:\n",
    "                                                                    network.each_request_virtual_paths_include_subpath[k][sub_path_id] = []\n",
    "                                                                except:\n",
    "                                                                    network.each_request_virtual_paths_include_subpath[k]={}\n",
    "                                                                    network.each_request_virtual_paths_include_subpath[k][sub_path_id] = []\n",
    "\n",
    "\n",
    "                                            for t in work_load.T:\n",
    "                                                if number_of_storages!=0:\n",
    "                                                    for j in network.storage_pairs:\n",
    "                                                        for p_s in network.each_request_real_paths[j]:\n",
    "        #                                                     print(\"for time %s and storage pair %s and real sub path %s\"%(t,j,p_s))\n",
    "                                                            for k in work_load.each_t_requests[t]:\n",
    "        #                                                         print(\"this is the request \",k)\n",
    "                                                                if k!=j:\n",
    "        #                                                             print(\"which was not equal to storage pair\",t,j,p_s,k)\n",
    "        #                                                             print(\"virtual including subpath\",network.each_request_virtual_paths_include_subpath[k])\n",
    "        #                                                             print(\"network.each_request_virtual_paths[k]\",network.each_request_virtual_paths[k])\n",
    "                                                                    for p in network.each_request_virtual_paths_include_subpath[k][p_s]:\n",
    "                                                                        if p not in network.each_request_virtual_paths[k]:\n",
    "                                                                            import pdb\n",
    "                                                                            print(\"ERROR storages %s time %s request %s has a path %s for subpaths but it is not in his virtual paths %s\"%(number_of_storages,t,k,p,p_s))\n",
    "                                                                            #print(network.each_request_real_paths[k])\n",
    "                                                                            print(\"virtual paths including subpath list \",network.each_request_virtual_paths_include_subpath[k][p_s])\n",
    "                                                                            print(\"virtual paths\",network.each_request_virtual_paths[k])\n",
    "                                                                            pdb.set_trace()\n",
    "\n",
    "                                            import pdb\n",
    "\n",
    "\n",
    "                                            \"\"\"we set the capacity of each storage node\"\"\"\n",
    "\n",
    "                                            network.set_storage_capacity()\n",
    "\n",
    "                                            \"\"\"we add new storage pairs as our user pairs and set the demand for them zero\"\"\"\n",
    "\n",
    "                                            work_load.set_storage_pairs_as_user_pairs(network.storage_pairs)\n",
    "\n",
    "\n",
    "                                            \"\"\"we set the fidelity threshold of each new storage pair as a user request\"\"\"\n",
    "                                            network.set_each_path_basic_fidelity()\n",
    "                                            work_load.set_threshold_fidelity_for_request_pairs(network.each_t_user_pairs,network.storage_pairs,network.each_user_request_fidelity_threshold)\n",
    "\n",
    "                                            \"\"\"we set the required EPR pairs to achieve each request threshold fidelity\"\"\"\n",
    "                                            network.set_required_EPR_pairs_for_path_fidelity_threshold()\n",
    "\n",
    "\n",
    "                                            \"\"\"solve the optimization\"\"\"\n",
    "        #                                     \n",
    "\n",
    "                                            try:\n",
    "                                                objective_value=-100\n",
    "                                                try:\n",
    "                                                    #objective_value,each_inventory_per_time_usage,each_time_each_path_delivered_EPRs,each_time_each_path_purification_EPRs = CPLEX_resource_cinsumption_minimization(network,work_load,life_time,i,cyclic_workload,storage_capacity)\n",
    "                                                    if number_of_storages==0:\n",
    "                                                        feasibility_objective_value,each_t_each_p_each_w = CPLEX_feasibility(network,work_load,life_time,i,cyclic_workload,storage_capacity)\n",
    "                                                        print(\"feasibility: for topology %s iteration %s from %s spike mean %s capacity %s  fidelity range %s  life time %s storage %s and path number %s objective_value %s\"%\n",
    "                                                        (network_topology,i,experiment_repeat, spike_mean,storage_capacity,fidelity_threshold_range,life_time, number_of_storages,num_paths, feasibility_objective_value))  \n",
    "                                                        if feasibility_objective_value>0:\n",
    "                                                            if number_of_storages==0:\n",
    "                                                                permited_work_load = True\n",
    "                                                    if permited_work_load:\n",
    "                                                        objective_value,each_inventory_per_time_usage,each_time_each_path_delivered_EPRs,each_time_each_path_purification_EPRs,solver_flag=maximizing_avg_fidelity(network,work_load,life_time,i,storage_capacity,max_allowed_purifiying_EPRs)\n",
    "                                                        current_available_fidelities.append(objective_value)\n",
    "                                                        if not solver_flag:\n",
    "                                                            if number_of_storages==0:\n",
    "                                                                each_t_fidelities = []\n",
    "                                                                for t,k_paths_ws in each_t_each_p_each_w.items():\n",
    "                                                                    for k,paths_ws in k_paths_ws.items():\n",
    "                                                                        served_EPRs = 0\n",
    "                                                                        this_k_purified_fidelities = []\n",
    "                                                                        for p,w in paths_ws.items():\n",
    "                                                                            purified_fidelity = network.each_path_basic_fidelity[p]\n",
    "                                                                            #print(\"purified_fidelity \",purified_fidelity)\n",
    "                                                                            weighted_purified_fidelity = w*purified_fidelity\n",
    "                                                                            #print(\"weighted_purified_fidelity \",weighted_purified_fidelity)\n",
    "                                                                            this_k_purified_fidelities.append(weighted_purified_fidelity)\n",
    "                                                                            served_EPRs = served_EPRs+w\n",
    "                                                                    #each_t_fidelities.append(sum(this_k_purified_fidelities)/each_t_each_request_demand[t][k])\n",
    "                                                                    each_t_fidelities.append(sum(this_k_purified_fidelities)/served_EPRs)\n",
    "                                                                objective_value = sum(each_t_fidelities)/len(each_t_fidelities)\n",
    "                                                                current_available_fidelities.append(objective_value)\n",
    "                                                        else:\n",
    "                                                            current_available_fidelities.append(objective_value)\n",
    "                                                        objective_value = max(current_available_fidelities)\n",
    "                                                    else:\n",
    "                                                        if number_of_storages==0:\n",
    "                                                            permited_work_load = False\n",
    "                                                        objective_value = -100\n",
    "\n",
    "                                                except ValueError:\n",
    "                                                    print(ValueError)\n",
    "                                                    #pass\n",
    "\n",
    "                                                \n",
    "                                                print(\"fidelity: for topology %s iteration %s from %s spike mean %s capacity %s  fidelity range %s  life time %s storage %s and path number %s objective_value %s\"%\n",
    "                                                    (network_topology,i,experiment_repeat, spike_mean,storage_capacity,fidelity_threshold_range,life_time, number_of_storages,num_paths, objective_value))  \n",
    "                                                if permited_work_load:\n",
    "                                                    with open(results_file_path, 'a') as newFile:                                \n",
    "                                                        newFileWriter = csv.writer(newFile)\n",
    "                                                        newFileWriter.writerow([network_topology,number_of_storages,num_paths,\n",
    "                                                                                life_time,\n",
    "                                                                                objective_value,spike_mean,num_spikes,i,\n",
    "                                                                                storage_node_selection_scheme,\n",
    "                                                                                fidelity_threshold_range,cyclic_workload,\n",
    "                                                                                distance_between_users,storage_capacity]) \n",
    "                                                    if objective_value>0 and number_of_storages>0:\n",
    "                                                        for storage_pair,t_saved_EPRs in each_inventory_per_time_usage.items():\n",
    "                                                            for t ,EPRs in t_saved_EPRs.items():\n",
    "                                                                this_slot_demands = work_load.get_each_t_whole_demands(t,network.each_t_user_pairs[t])\n",
    "                                                                with open(inventory_utilization_results_file_path, 'a') as newFile:                                \n",
    "                                                                    newFileWriter = csv.writer(newFile)\n",
    "                                                                    newFileWriter.writerow([network_topology,number_of_storages,\n",
    "                                                                    num_paths,i,life_time,storage_pair,t,EPRs,spike_mean,\n",
    "                                                                    num_spikes,storage_node_selection_scheme,this_slot_demands,\n",
    "                                                                    fidelity_threshold_range,\n",
    "                                                                    cyclic_workload,distance_between_users,storage_capacity])\n",
    "                                                        for t,k_delived_EPRs in  each_time_each_path_delivered_EPRs.items():\n",
    "                                                            this_slot_demands = work_load.get_each_t_whole_demands(t,network.each_t_user_pairs[t])\n",
    "                                                            for k,delived_EPRs in k_delived_EPRs.items():\n",
    "                                                                purification_EPRs = each_time_each_path_purification_EPRs[t][k]\n",
    "                                                                with open(delived_purification_EPRs_file_path, 'a') as newFile:                                \n",
    "                                                                    newFileWriter = csv.writer(newFile)\n",
    "                                                                    newFileWriter.writerow([network_topology,number_of_storages,\n",
    "                                                                    num_paths,i,t,life_time,spike_mean,\n",
    "                                                                    num_spikes,storage_node_selection_scheme,this_slot_demands,\n",
    "                                                                    fidelity_threshold_range,\n",
    "                                                                    cyclic_workload,distance_between_users,storage_capacity,\n",
    "                                                                    k,delived_EPRs,purification_EPRs])\n",
    "\n",
    "                                            except ValueError:\n",
    "                                                #pass\n",
    "                                                print(ValueError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_repeat =400 #indicates the number of times that we repeat the experiment\n",
    "spike_means = [40] # list of mean value for spike model traffic generation\n",
    "num_spikes = 2 # shows the number of nodes that have spike in their demand. Should be less than the number of user pairs\n",
    "topology_set = sys.argv[1] # can be either real, random1, or random2\n",
    "\n",
    "setting_demands = sys.argv[2] # indicates the way we generate the demands. python_library for using tgem library. ransom for generating a random demand\n",
    "max_allowed_purifiying_EPRs = int(sys.argv[3])\n",
    "if setting_demands not in [\"random\",\"python_library\"]:\n",
    "    print(\"please run the script by python IBM_cplex_feasibiloty.py real/random1/random2 random/python_library\")\n",
    "else:\n",
    "    storage_node_selection_schemes=[\"Degree\",\"Random\"]\n",
    "    storage_node_selection_schemes=[\"Degree\"]\n",
    "    cyclic_workload = \"sequential\"\n",
    "    storage_capacities = [400,500,200,600,100]\n",
    "    storage_capacities = [500]\n",
    "    fidelity_threshold_ranges = [0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,1.0]\n",
    "    fidelity_threshold_ranges = [0.8]\n",
    "    distance_between_users = 2\n",
    "\n",
    "    given_life_time_set = [1000,2]# 1000 indicates infinite time slot life time and 2 indicates one time slot life\n",
    "    number_of_user_pairs =1\n",
    "    number_of_time_slots = 5\n",
    "    results_file_path = 'results/results_maximizing_fidelity.csv'\n",
    "    inventory_utilization_results_file_path = 'results/inventory_maximizing_fidelity_utilization.csv'\n",
    "    delived_purification_EPRs_file_path = 'results/delived_purification_EPRs_file_path_maximizing_fidelity.csv'\n",
    "    each_network_topology_file = {}\n",
    "    if topology_set ==\"real\":\n",
    "        each_network_topology_file = {\"ATT\":'data/ATT_topology_file',\"Abilene\":'data/abilene',\"SURFnet\":'data/Surfnet',\"IBM\":'data/IBM'}\n",
    "\n",
    "    elif topology_set ==\"random1\":\n",
    "        for i in [2,4,6]:\n",
    "            each_network_topology_file[\"random_erdos_renyi2_\"+str(i)]= \"data/random_erdos_renyi2_\"+str(i)+\".txt\"\n",
    "            each_network_topology_file[\"random_barabasi_albert2_\"+str(i)]= \"data/random_barabasi_albert2_\"+str(i)+\".txt\"\n",
    "    # each_network_topology_file = {\"Testing_topology\":'data/test_topology'}\n",
    "    elif topology_set ==\"random2\":\n",
    "        for i in range(1,4):\n",
    "            each_network_topology_file[\"random_erdos_renyi_\"+str(i)]= \"data/random_erdos_renyi_\"+str(i)+\".txt\"\n",
    "            each_network_topology_file[\"random_barabasi_albert_\"+str(i)]= \"data/random_barabasi_albert_\"+str(i)+\".txt\"\n",
    "    elif topology_set ==\"big\":\n",
    "        for i in range(2):\n",
    "            each_network_topology_file[\"random_erdos_renyi_\"+str(i)]= \"data/size_\"+str(400)+\"_random_erdos_reni_0_0_1_\"+str(i)+\".txt\"\n",
    "    elif topology_set ==\"linear\":\n",
    "        for topology_size in [30,20]:\n",
    "            each_network_topology_file[\"linear_chain_\"+str(topology_size)]= \"data/linear_topology_\"+str(topology_size)+\".txt\"\n",
    "        \n",
    "    # import networkx as nx\n",
    "    # all_diameters = []\n",
    "    # for topology,file in each_network_topology_file.items():\n",
    "    #     print(\"for topology\",topology)\n",
    "\n",
    "    #     g = nx.Graph()\n",
    "    #     f = open(file, 'r')\n",
    "    #     header = f.readline()\n",
    "    #     for line in f:\n",
    "    #         line = line.strip()\n",
    "    #         link = line.split('\\t')\n",
    "    #         i, s, d, c = link\n",
    "    #         g.add_edge(int(s),int(d),weight=1)\n",
    "    #     f.close()  \n",
    "    #     all_diameters.append(nx.diameter(g))\n",
    "\n",
    "    maximizing_fidelity(each_network_topology_file,results_file_path,inventory_utilization_results_file_path,delived_purification_EPRs_file_path,number_of_user_pairs,number_of_time_slots,spike_means,num_spikes,experiment_repeat,storage_node_selection_schemes,fidelity_threshold_ranges,cyclic_workload,storage_capacities,given_life_time_set,distance_between_users,setting_demands,max_allowed_purifiying_EPRs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
